<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>PSY 503: Foundations of Statistical Methods in Psychological Science</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jason Geller, Ph.D. (he/him/his)" />
    <meta name="date" content="2022-09-04" />
    <script src="04_Measurement_files/header-attrs/header-attrs.js"></script>
    <link href="04_Measurement_files/countdown/countdown.css" rel="stylesheet" />
    <script src="04_Measurement_files/countdown/countdown.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# PSY 503: Foundations of Statistical Methods in Psychological Science
## Measurement
### Jason Geller, Ph.D. (he/him/his)
### Princeton University
### 2022-09-04

---







---
# Today

- What is measurement?

- Scales of measurement

- Reliability and validity
  
- Measurement in practice: listening effort
---
class: inverse center middle

&gt; "Whatever exists at all exists in some amount. To know it thoroughly involves knowing its quantity as well as its quality.”
  -Edward L. Thorndike
  
???

Welcome back, gang. Today we are going to talk about measurement. Measurement is probably the most fundamental part of doing science. For instance, as a cognitive scientist I am interested in learning and memory. If I wanted to know how learning increased over time or how many items are stored in memory and for how long I would have measure it. Knowing how to measure them is critical knowing about the thing itself. 

---
# Measurement

&gt; The assignment of scores so that the scores represent some characteristic of the individuals

&lt;img src="weightheight.jpeg" width="50%" style="display: block; margin: auto;" /&gt;

???

A formal definition from your book is: 
---
# What things do we want to measure?

.pull-left[

- Depression
- Effort
- Intelligence
- Memory
- Social support
- Extroversion
- Eating behavior
- Parent child relationships
- Attention
- Burn out
- Hopelessness

]

.pull-right[
&lt;img src="poppins.png" width="50%" style="display: block; margin: auto;" /&gt;
]

???

A lot of what we want to study in the cognitive sciences cannot be measured easily--they defy direct observation. So we could use a measuring tape to get our heights but we could not use it to measure personality, unless we are Mary poppins. To examine constructs, psychologists do many things. They observe, they get ratings, ask people to fill out surveys. 

There are multiple ways to do this. We usually give them tasks to do or we ask them to fill out surveys. 

---
# Constructs ≠ Variables

&lt;img src="latent.png" width="70%" style="display: block; margin: auto;" /&gt;

???

constructs are these big broad mental abstractions

we call these latent constructs

To study them we need ways to operationalize them by turning them into numbers. For depression we can base it on a score or rating  ore self-reported. We care about the scores bc we think they tell us about the construct. 

Important to know here. Super important
constructs are not variables. We use variables to get at the constructs. It is hard to understand this bc the media talks about constructs all the time.   
---
# Scales of Measurement

- Variables are defined and categorized four ways: 

1. Nominal -&gt; categorical data 
2. Ordinal
3. Interval
4. Ratio

**NOIR**

---
# Nominal

- Nominal ≈ name, so numbers on a nominal scale just name or stand for a category or individual

- Numerals arbitrarily assigned to name events/objects

– Given 2 nominal measurements: 

  - Can determine whether the same or not
  - Not able to tell if one has more or less of measured attribute
  
- Examples: gender, experimental vs control group
---
# Ordinal

– Data have characteristics of nominal scale + more

– Numbers indicate the ordering of individuals on some dimension

- Given 2 ordinal measurements:

  - Can state whether they have equal amounts of the attribute or not
  - May not be able to make statements about the difference between the pair of scores
  
- Examples: Ranking on a task, rating your preferences

---
# An Olympic Example

&lt;img src="gold.png" width="70%" style="display: block; margin: auto;" /&gt;
  
---
# Interval 

- Data have characteristics of ordinal scale +  more

- Intervals have consistent meaning – equal  units

- No true zero

- Example: temperature in °F

---
# Ratio

- Intervals have consistent meaning –  equal units

– True zero 

– Ratios are meaningful

– Examples: frequencies, times, rates
---
# An Olympic Example

&lt;img src="gold.png" width="70%" style="display: block; margin: auto;" /&gt;
---
Knowledge Check

&lt;iframe src="https://app.sli.do/event/hvreh8tr" height="100%" width="100%" frameBorder="0" style="min-height: 560px;" title="Slido"&gt;&lt;/iframe&gt;
---

---
# Reliability

&gt; How consistent or how precise a measure/method is

  - Test-Retest (over time)
  
  - Internal (across time)
  
  - Inter-rater  (between different researchers)
  
---

# Reliability

&gt; Consistency of a measure: 

  - Test-Retest (across time)
  
&lt;img src="testretest.png" width="50%" style="display: block; margin: auto;" /&gt;
---
# Reliability

&gt; Consistency of a measure: 

  - Test-retest (across time)

&lt;iframe src="https://www.proprofs.com/quiz-school/story.php?title=mte2mdawoq2srl" width="50%" height="400px" data-external="1"&gt;&lt;/iframe&gt;
---
# Reliability

&gt; Consistency of a measure:
  
  - Internal (across items)
  
  1. I love Halloween. **Agree**
  2. I feel happy when I decorate my house for Halloween. **Agree**
  3. I feel angry when the Halloween season is approaching. **Disagree**
  
    - Cronbach's `\(\alpha\)`
      - .8  
---
# Reliability

&gt; Consistency of a measure:
  
  - Interrater (across different researchers)
---
# Validity

&gt;  Accuracy [e.g., are we really measuring what we think we are measuring?]
  
  - Face validity
  
&gt;  The extent to which a measurement method appears “on its face” to measure the construct of interest

  - Criterion or convergent validity
&gt; The extent to which people’s scores on a measure are correlated with other variables (known as criteria) that one would expect them to be correlated with.

  - Discriminate  or divergent validity
&gt; The extent to which scores on a measure of a construct are not correlated with measures of other, conceptually distinct, constructs and thus discriminate between them.

---
# Bank Robbery

- Eyewitness memory plays an important role in helping police solve crimes. However, people’s abilities to accurately recall what they saw can substantially impact whether a criminal is convicted—and equally, if an innocent person is wrongfully convicted. So, it’s important to get it right. 

- First, let’s find out how well you can remember what happens during a bank robbery

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/1TkSy_e5WTg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

---
# Viewing #1

- You have now viewed a clip of a simulated bank robbery.

Take a few minutes to write down your description of the main offender

---
# Work with partner

Take a few minutes to write down your description of the main offender

- Inter-observer agreement = `# agreements X 100 / # agreements + # disagreements`
For example: if there were 5 agreements and 3 disagreements… 
[a] 5 x 100 = 500
[b] 5 + 3 = 8
[c] 500/8 = 62.5% inter-observer agreement

- What percentage agreement did you end up with? What do you think it says about the reliability of the instructions you were given?

<div class="countdown" id="timer_752a4be5" data-update-every="1" tabindex="0" style="right:0;bottom:0;">
<div class="countdown-controls"><button class="countdown-bump-down">&minus;</button><button class="countdown-bump-up">&plus;</button></div>
<code class="countdown-time"><span class="countdown-digits minutes">05</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---
# Group Discussion

- What sort of percentage agreements did we get?
- What do you think these percentages tell us about the reliability of the instructions you were given?
---
# Viewing #2

- We will watch the video again. 
- Then you will get new instructions for describing the offender. 
- Then you will work with your partner to calculate inter-rater reliability again

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/1TkSy_e5WTg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
---
# Viewing #2

Using the checklist below, describe the main offender: 

- Were they male or female?
- What was their hair colour?
- What was their skin colour?
- What colour were their eyes?
- What was the colour of their shirt?
- Were they wearing a jacket? If so, what colour was it?
- Were they wearing long pants, jeans, or shorts?
- What colour was their pants/jeans/shorts?
- Were they wearing glasses?
- Were they wearing a hat? 
- Were they wearing a balaclava?
- Did he/she have a gun?
- Did he/she have a knife?
- Were they carrying anything? If yes, what was it?

---
# Work with partner
Take a few minutes to write down your description of the main offender

- Inter-observer agreement = `# agreements X 100 / # agreements + # disagreements`

- What percentage agreement did you end up with? What do you think it says about the reliability of the instructions you were given?

&gt; For example: if there were 5 agreements and 3 disagreements
 [a] 5 x 100 = 500
 [b] 5 + 3 = 8
 [c] 500/8 = 62.5% inter-observer agreement
 
<div class="countdown" id="timer_9873f95e" data-update-every="1" tabindex="0" style="right:0;bottom:0;">
<div class="countdown-controls"><button class="countdown-bump-down">&minus;</button><button class="countdown-bump-up">&plus;</button></div>
<code class="countdown-time"><span class="countdown-digits minutes">05</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---
# Viewing #3

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/1TkSy_e5WTg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
---
Using the checklist below, describe the main offender
- Were they male or female?
- What was their hair colour?
- What was their skin colour?
- What colour were their eyes?
- What was the colour of their shirt?
- Were they wearing a jacket? If so, what colour was it?
- Were they wearing long pants, jeans, or shorts?
- What colour was their pants/jeans/shorts?
- Were they wearing glasses?
- Were they wearing a hat? 
- Were they wearing a balaclava?
- Did he/she have a gun?
- Did he/she have a knife?
- Were they carrying anything? If yes, what was it?

---
# On your own this time

Now, using the same formula, calculate how well you agree within yourself (test-retest reliability). That is, what is the level of correspondence between your observations at Viewing #2 and your observations at Viewing #3?

- Inter-observer agreement = `# agreements X 100 / # agreements + # disagreements`

<div class="countdown" id="timer_7f36f385" data-update-every="1" tabindex="0" style="right:0;bottom:0;">
<div class="countdown-controls"><button class="countdown-bump-down">&minus;</button><button class="countdown-bump-up">&plus;</button></div>
<code class="countdown-time"><span class="countdown-digits minutes">05</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---
# Group Discussion

How do these results compare with the results for the inter-rater reliability calculations, i.e., are they different? In what way? Any ideas why or why not?


---
background-image: url(listeningeffort.png)
background-position: center
background-size: cover
---
# Listening Effort

- Conceptual definition

&gt; The deliberate allocation of mental resources to overcome obstacles in goal pursuit when carrying out a listening task

&lt;img src="list.png" width="70%" style="display: block; margin: auto;" /&gt;
---
# Operationalization

- How do we measure "accuracy"?

&lt;img src="acc.png" width="70%" style="display: block; margin: auto;" /&gt;
---
# Operationalization

- How do we measure "effort"

  - Self-report:

“how hard did you have to work mentally to accomplish your level of performance?”

&lt;img src="selfeffort.png" width="70%" style="display: block; margin: auto;" /&gt;

---
# How do we measure "effort"

.pull-left[

- Physiological measures

  - Pupil size
  - Skin conductance (GSR)
  - Heart rate
]

--
.pull-right[
&lt;img src="pupil.png" width="70%" style="display: block; margin: auto;" /&gt;
]
---
# How do we measure "effort"

.pull-left[

- Behavioral measures

  - Recall
]


.pull-right[
&lt;img src="behaveeffort.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---
# How do we measure "effort"?

.pull-left[

- Behavioral measures

  - dual-task
]

.pull-right[
&lt;img src="dual.png" width="100%" style="display: block; margin: auto;" /&gt;
]
---
# How many measures of listening effort in the literature?

- 24!
---
# Measuring Listening Effort (Strand et al., 2018)

&lt;img src="corr1.png" width="100%" style="display: block; margin: auto;" /&gt;

--

- *r* - .2 
---
# Jingle and Jangle Fallacy (Thorndike, 1904; Kelley, 1927; Flake &amp; Fried, 2020)


- Jingle 

&gt; Falsely assuming that two tasks measure the same construct because they have the same name

---
# Measuring Listening Effort (Strand et al., 2018)

&lt;img src="corr2.png" width="100%" style="display: block; margin: auto;" /&gt;

--


---
# Jingle and Jangle Fallacy (Thorndike, 1904; Kelley, 1927; Flake &amp; Fried, 2020)

- Jingle 

&gt; Falsely assuming that two tasks measure the same construct because they have the same name

- Jangle 

&gt; Falsely assuming that two tasks measure different constructs because they have different names
---

# Key Points

- Constructs ≠ variables

- Constructs are measured multiple ways, which may lead to different outcomes

- Thinking carefully about measurement is fundamental to understanding replication
---
&lt;img src="timeline.jpeg" width="100%" style="display: block; margin: auto;" /&gt;
---
# Group Work

- What are the two key constructs of interest (across the three studies)?
- How were the constructs measured?
- How did the authors select the measures and where do the measures come from?
- How were the measurement decisions in these studies described and justified?


&lt;img align="left" width="200" height="200" src="study2.png"&gt;
&lt;img align="right" width="200" height="200" src="study5.png"&gt;
&lt;img align="center" width="200" height="200" src="study6.png"&gt;

---
# The Four Horsemen of Validity

- Construct validity

- Statistical conclusion validity

- Internal validity

- External validity

---
# Contruct Validity

- A new program hopes to&lt;br&gt;improve student commitment to school]

--
  - Participants score 200 points higher on the SAT and have a 0.3 higher GPA, on average

.box-3.medium[Success!]&amp;ensp;&amp;ensp;.box-3.medium[Success?]


---
# Construct validity

&gt; Are you measuring what you want to measure?]

--

- Do test scores measure commitment to school?&lt;br&gt;Teacher performance? Principal skill?]

--

- Test scores measure how good kids are at taking tests]

--

This is why we spend so much time on outcome measurement construction!

---

layout: false
name: statistical-conclusion-validity
class: center middle section-title section-title-1 animated fadeIn

# Statistical conclusion&lt;br&gt;validity

---
# Power




.box-inv-1[A training program causes incomes to rise by $40]

.center.small[

|Person |Group     | Before | After  | Difference |
|:------|:---------|:------:|:------:|:----------:|
|295    |Control   | 122.09 | 229.04 |   106.95   |
|126    |Treatment | 205.60 | 199.84 |   -5.76    |
|400    |Control   | 133.25 | 130.40 |   -2.85    |
|94     |Treatment | 270.11 | 206.56 |   -63.54   |
|250    |Control   | 344.37 | 222.89 |  -121.49   |
|59     |Treatment | 312.41 | 268.06 |   -44.35   |
]

---

# Power

.pull-left[
.box-1.small[Survey 10 participants]

&lt;img src="04_Measurement_files/figure-html/power-small-1.png" width="100%" /&gt;

]

--

.pull-right[
.box-1.small[Survey 200 participants]

&lt;img src="04_Measurement_files/figure-html/power-big-1.png" width="100%" /&gt;

]

---

# What's the right sample size?

.box-inv-1[Use a statistical power calculator to&lt;br&gt;make sure you can potentially detect an effect]

.center[
&lt;figure&gt;
  &lt;img src="power-search.png" alt="Google power calculator" title="Google power calculator" width="50%"&gt;
&lt;/figure&gt;
]

---

# Test assumptions

.box-inv-1[Every statistical test has certain assumptions]

--

.box-1.smaller[For instance, for OLS:]

.center.float-left.smaller[
.box-1[Linearity]&amp;ensp;.box-1[Homoscedasticity]&amp;ensp;.box-1[Independence]&amp;ensp;.box-1[Normality]
]

--

.box-inv-1.medium.sp-before[Make sure you're doing the stats correctly]

---

# Fishing and p-hacking

.box-inv-1[Wouldn't it be awesome to run thousands of models&lt;br&gt;with different combinations of variables&lt;br&gt;until you find coefficients that are statistically significant?]

--

.box-1.large[Don't!]

--

.center[
&lt;figure&gt;
  &lt;img src="p-hacking.png" alt="p-hacking" title="p-hacking" width="60%"&gt;
&lt;/figure&gt;
]

???

&lt;https://projects.fivethirtyeight.com/p-hacking/&gt;

---

# Spurious statistical significance

.box-inv-1.sp-after[If *p* threshold is 0.05 and you measure 20 outcomes,&lt;br&gt;1 will likely show correlation by chance]

--

.center[
&lt;figure&gt;
  &lt;img src="img/06/xkcd.png" alt="xkcd: significance" title="xkcd: significance" width="60%"&gt;
&lt;/figure&gt;
]

???

&lt;https://xkcd.com/882/&gt;

---

layout: false
name: internal-validity
class: center middle section-title section-title-5 animated fadeIn

# Internal Validity

&gt;  Extent to which you can be confident that a cause-and-effect relationship established in a study cannot be explained by other factors.

---
# Threats to Internal Validity

Shaddish, Cook, and Campbell (2001) list 8 threats: 

- Ambiguous Temporal Precedence
- Selection 
- Maturation
- History
- Regression
- Attrition
- Testing
- Instrumentation

---
# Maturation
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

  &gt; Participants are changing  naturally over time betweenpre-test and post-test. Change  is not due to the IV.


- Rule out by using a control/comparison group.

---

# History
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

  &gt; An event intervenes between  pre-test and post-test to change participants. Changes are not due to the IV.


- Maybe something big happened in the news that was related to your study.

- Rule out by using a  control/comparison group.

---

# Regression to the mean

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&gt; Participants are selected for intervention based on their extreme scores. On re-measurement, their scores become more moderate, but changes are not due to the IV

  - A group's average is extremely depressed at pretest, in part because some members volunteered for therapy when they were feeling worse


- Rule out by using a control/comparison group.

---

# Attrition
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&gt; Participants drop out of thestudy at different rates from  treatment group and control group

--
Maybe they got: 

- Illness
- Died
- Left the experiment

---

# Testing

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&gt; Participants’ responses change over repeated testing. Changes are not due to the IV

  - GRE verbal scores improve only becasue students take the same ersion of the test both times

- Solve by using a between-subjects, post-test only design 

---
# Instrumentation

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&gt; The meaning of a measuring instrument changes over repeated use. Changes are not due to the IV.


- Coders become bored or fatigued over time


- Solve by using masked coders, randomly assigning coders to stimuli,, or training
---
# Threats to generalization (or particularization)  

- Individual vs. Average causal effects  


- External validity  
---
# Individual vs. Average causal effects  

- Average causal effects cannot be particularized to any single individual 

  - Unless additional assumption: constant treatment effect  
  
- Important consideration in clinical and health contexts  
---
# External validity  

- A single study = one piece of evidence for the existence of an effect within constrains of:  

  - Population  
  
  - Treatments  
  
  - Outcome measures 
  
  - Settings 
  
  - Period 
  

- Be cautious  

---
# Observational Design


- Relative to the experimental design, observational designs have weaker internal validity (less control) but stronger external validity (more naturalistic)

  - Observational design usually requires longitudinal data that comprise both pretest and post-test time periods
  - Key to a good observational design is high-quality comparison, so one must think really hard about the control group and document equivalence

---
# Observational Design

 - __Natural experiment__: Naturally occurring event with pseudo-random variation in treatment status
    - Unplanned intervention (“shock”): Mother Nature and government policy changes are frequently the source of natural experiments
    - Assignment is outside the control of the investigator, but also of the participants

- __Quasi-experiment__: Units “choose” whether they receive treatment, but that choice can sometimes be controlled via design and statistical adjustments
      - Assignment is fully controlled by the participants
      - Chief threat to internal validity is selection bias (i.e., endogenous treatment assignment)
---
#
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": true,
"background-image": "url(\"lover.png\")",
"background-size": "cover"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
