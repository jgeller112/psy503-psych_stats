---
title: "PSY 503: Foundations of Statistics in Psych Science"
subtitle: "Introduction to Probability"
institute: "Princeton University"
author: "Jason Geller, Ph.D. (he/him/his)"
date:  '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 18:10
      countIncrementalSlides: true
      background-image: url("lover.png")
      background-size: cover
---

```{r setup, include=FALSE}

library(tidyverse)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "36%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  fig.show = TRUE,
  hiline = TRUE
)

hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(tidyverse)

style_solarized_dark(
  header_font_google = google_font("Aleo"),
  header_h1_font_size = "36px",
  header_color = "black",
  text_font_google = google_font("Aleo"),
  text_font_size = "28px",
  text_color = "black", 
  background_color = "orange", 
  code_font_google = google_font("Share Tech Mono"),
  extra_css = list(
    ".remark-slide-content h2" = list(
      "margin-top" = "2em",
      "margin-bottom" = "2em"
    ),
    .big = list("font-size" = "150%"),
    .small = list("font-size" = "75%"),
    .subtle = list(opacity = "0.6"),
    ".countdown-has-style h3, .countdown-has-style h3 ~ p, .countdown-has-style h3 ~ ul" = list(
      "margin" = "0"
    ),
    ".countdown-has-style pre" = list(
      "margin-top" = "-10px"
    ),
    "p .remark-inline-code" = list(
      "background-color" = "orange",
      "padding" = "2px 2px",
      "margin" = "0 -2px"
    ),
    blockquote = list("margin-left" = 0),
    "em" = list(color = "#2aa198")
  ),
)

```
# Knowledge Check

<div style='position: relative; padding-bottom: 56.25%; padding-top: 35px; height: 0; overflow: hidden;'><iframe sandbox='allow-scripts allow-same-origin allow-presentation' allowfullscreen='true' allowtransparency='true' frameborder='0' height='315' src='https://www.mentimeter.com/app/presentation/a545260831446a899d989a08b7445b38/34954ecab6bd/embed' style='position: absolute; top: 0; left: 0; width: 100%; height: 100%;' width='420'></iframe></div>

---

# Last Class

- Measurement is hard, but so important

- Make sure you understand different types of reliability: 
  - Test-retest
  - Internal
  - Inter-rater
  
- Make sure you understand different types of validity:
 - Construct
    - Face
    - Convergent
   - Divergent
---
# Today

- What is Probability?

- Different ways of thinking about probability

- Rules of probability

---
# Probability Warm-up

1. What is probability of drawing the ace of spades from a fair deck of cards?

2. What is the probability of drawing an ace of any suit?

3. You are going to roll some dice twice. What is the chance you roll double 1s?

4. What is the chance that a live specimen of New Jersey Devil will be found?

5. Who is more likely to be a victim of a street robbery, a young man or an old lady?

6. Yesterday the whether forecaster said that there was a 30% chance of rain today, and it rained today. Was she right or wrong?
---
# What is Probability Theory? 

```{r, fig.align='center', echo=FALSE, warning=FALSE,  out.height="20%", out.width = "80%"}
library(flextable)
knitr::include_graphics("prob.JPG")
```
---
# What is Probability Theory?

<br>
<br> 

Probability theory is the study of __random processes__

- Probability is used to characterize uncertainty/randomness

```{r, fig.align='center', echo=FALSE, warning=FALSE,  out.width = "50%"}

knitr::include_graphics("probability-line.svg")
```

---
# Random Processes: Intuition

.pull-left[

- Let's flip a fair coin  

```{r}

set.seed(973)

coinflips <- function(x) {
    flip <- rbinom(x, 1, 0.5)
    flip <- ifelse(flip==1, "Tails", "Heads")
    return(flip)
}


```

  1.   Can you tell me what the outcome will be?   

  2.  If we were to flip a fair coin many many times, would you be able to tell the proportion of times that we would obtain heads?  
]
--
.pull-right[
<br>
<br>
- If answer to first question is "NO" 

AND
- Answer to second question is "YES"   

THEN  

- You are dealing with a random process  
]

---
# Definition

<br>
<br>

> Random processes are __mechanisms__ that produce outcomes...  from __a world/set of possible__ outcomes...  with some degree of __uncertainty__ but with __regularity__.  

---
# Probability Terminlogy  

- __Experiment__ or __Trial__:
    - Any activity that produces or observes an outcome

- __Sample space:__ $\Omega$ 

  - The set of all possible outcomes

- __Outcome:__ $\omega$  

  - Possible realization of the random process
    - heads

- __Event:__ $A$, $B$, $C$, etc. 

  - A given outcome or set of outcomes
  
- __Probability__: Proportion of outcomes favoring an event  
---
# Examples of Random Processes

- Random assignment of $N$ individuals to an experimental condition    

--

<br>

- Random draw of a sample of $n$ individuals from a population of $N$ individuals  

--

<br>

- Rolling a die  

---
# Illustration: Random Assignment 

- We randomly assigned an individual to a Treatment (T) vs. Control (C)  

  - Sample space?  

--

  - We could express $\Omega$ in the following ways:   

  - $\Omega = \{\mathrm{Treatment, \: Control\}}$  
  
  - $\Omega = \{\mathrm{T, \: C\}}$  
  
--

- What if we assigned two individuals to Treatment (T) vs. Control (C)

--

  - $\Omega = \{\mathrm{TT, \:TC, \:CT, \:CC}\}$   
---
# Events  

- An _event_ is a subset of the sample space $\Omega$ and corresponds to the realization of one or more than one outcomes $\omega$   
 
 
  - Let $\Omega = \{\mathrm{TT, \:TC, \:CT, \:CC}\}$   

- We could let $A$ be __event__ that both individuals are assigned to the same experimental condition  

- We could write:  

  - $A = \{TT, \: CC\}$ 

- Another example?  

---
# Notations   

```{r echo=FALSE,out.width="100%",fig.cap="",fig.show='hold',fig.align='center'}

knitr::include_graphics('images/prob_notation.png')

``` 
---
# Practice with Events  

- We randomly assign 8 participants to T vs. C
  
  - Possible outcome:

--

- $\omega =  \mathrm{TTTTCCTC}$

--

- Sample space: 

--

  - Set of all possible strings of length 8 of T's and C's  

---
# Practice with Events  

- Let's __randomly__ generate a possible outcome $\omega_j$ in R \vspace{.40cm}

```{r eval=FALSE}
sample(c("T", "C"), 
       size = 8, 
       replace = TRUE)
```

- In the background, does R draw from this sample space? 

--

- NO: Keep in mind that R draws an outcome $\omega_j$ from $\Omega = \{T, C\}$ 8 times in a row with replacement  

---
# Probability Warm-up

- What is probability of drawing the ace of spades from a fair deck of cards?

```{r}

ace=1/52

ace

```


- What is the probability of drawing an ace of any suit?

```{r}

ace=4/52

ace

```
---
-  You are going to roll some dice twice. What is the chance you roll double 1s?

```{r}
dice1s <- 1/6*1/6
dice1s

```
---
- What is the chance that a live specimen of New Jersey Devil will be found?
- 0%

5. Who is more likely to be a victim of a street robbery, a young man or an old lady?

- old lady

6. Yesterday the whether forecaster said that there was a 30% chance of rain today, and it rained today. Was she right or wrong?

- Depends
---

class: center, inverse

background-image: url("weather.png")
---
# Different Ways of Thinking About Probability

- Classic/Naive

  - **All outcomes are equally likely**

Let $A$ be an event with a finite sample space $\Omega$. The _naive probability_ of $A$ is 
\begin{equation}
P(A) = \frac{|A|}{|\Omega|} 
\end{equation}

in which |A| is the number of possible outcomes $\omega$ that satisfy A, and |$\Omega$| is the total number of possible outcomes $\omega$ within $\Omega$.  

---
# Dice Rolls
```{r, fig.align='center', out.width="100%", out.height="100%", echo=FALSE}

i <- 1:6
j <- sample(i, 1e6, replace = TRUE)
dice_roll <- tibble("Roll" = j, "Outcome" = j) %>%
  ggplot() +
  geom_bar(aes(x = Outcome), color = "blue", fill = "white") + labs(title = "Dice Rolls")
dice_roll
```
---
# Wait, why is this naive?  

- Requires $\Omega$ to be finite   

- Requires each possible outcome $\omega$ to have the same weight    

  - This can be misleading!  

---
# Wait, why is this naive?

Is the assumption of equal probability realistic? 

--

- $d_1$: Watching a horror movie

- $d_0$: Watching a neutral movie

- $Y$: Fear response measured 

  - Is their an equal probability of attrition in this study?
  
--

- Online data collection
---

# Different Ways of Thinking About Probability

-  Frequentist view

  -  **Past Performance**
  
  - Relative frequency -> Proportion of times an event occurred out of all occasions it could have occurred
  
  \begin{equation}
P(A) = \frac{|$f$|}{*N*} 
\end{equation}

- Where $f$ = frequency of outcome and $N$ = Total # 
- Over the long-run (many repetitions) what is the probability of X event?
  
---
# Different Ways of Thinking About Probability

- Empirical probability

  - Should we cross the bridge?

```{r, fig.align='center', echo=FALSE, warning=FALSE,  out.heigh="5%", out.width = "50%"}

knitr::include_graphics("bridge.png")
```

$$ P(death) = \frac{P(number of deaths)}{P(total)} $$
---
# Coin Flips

```{r, fig.align='center', echo=FALSE, warning=FALSE,  out.heigh="100%", out.width = "100%"}

knitr::include_url("https://seeing-theory.brown.edu/basic-probability/index.html")
```
---
# Globe Toss 

---
# Different Ways of Thinking About Probability

-  Bayesian (Personal belief)

  - In what realistic setting would we actually perform the same experiment infinite times?
  
  - Many probability questions concern the outcome of a singular trial rather than hypothetical repeated trials, and decision makers with the same information may differ
---
class: center, inverse

background-image: url("fightclub.jpeg")

# Rules of Probability

---
# Probability Rules 

- Probabilities take values between 0 and 1 (inclusive)   

  - For some event $A$: $$0 \leq P(A) \leq 1$$

- Probability cannot be negative    

- Probability cannot be greater than 1
---
# Probability Rule # 2  

- Since $\Omega$ is the entire sample space, $$P(\Omega) = 1$$  

- e.g.,If you belong to one of three political parties then the sum of P(R), P(D) and P(I) = 1

---
# Probability Rule #3 

- Complement

  - By definition $$ P(A) + P(A^c) = 1$$  

  - This implies $$ P(A^c) = 1 - P(A)$$

```{r, fig.align='center', echo=FALSE, warning=FALSE,  out.width = "50%"}

knitr::include_graphics("complement-venndiagram.svg")
```

---
# Probability Rule # 4

- Addition Rule: If A and B are two events in a probability experiment, then the probability that either one of the events will occur is:

.pull-left[

- Mutually Exclusive

P(A or B) = P(A) + P(B)
```{r, fig.align='center', echo=FALSE, warning=FALSE,  out.width = "50%"}

knitr::include_graphics("mutually-exclusive-venndiagram.svg")
```
]
.pull-right[
- Non-Mutually Exclusive
```{r, fig.align='center', echo=FALSE, warning=FALSE,  out.width = "50%"}

knitr::include_graphics("addition-rule-independent-venndiagram.webp")
```
]
---
# Practice

```{r, echo=FALSE}
m<-data.frame(
  stringsAsFactors = FALSE,
             Color = c("Brown", "Red", "Yellow", "Green", "Orange", "Blue"),
             Count = c(13L, 13L, 14L, 16L, 20L, 24L)
)

m %>%
  flextable()
```

*p*(blue or green)

--
```{r, echo=FALSE}
m<-data.frame(
  stringsAsFactors = FALSE,
             Color = c("Brown", "Red", "Yellow", "Green", "Orange", "Blue"),
             Count = c(13L, 13L, 14L, 16L, 20L, 24L)
)

m %>%
  flextable()
```

*p*(blue or green)
```{r}

24/100 + 16/100

```
---
# Union

> The union of two sets encompasses any element that exists in either one or both of them. We can represent this visually as a venn diagram as shown.

> ![](union-venndiagram.svg)

---
# Intersection

> The intersection between two sets encompasses any element that exists in BOTH sets and is often written out as


```{r, fig.align='center', echo=FALSE, warning=FALSE,  out.width = "50%"}
knitr::include_graphics("intersection-venndiagram.svg")
```
 - Joint probability

---
# Multiplication Rule

- The multiplication rule is used to find the probability of two events, *A* and *B*, happening simultaneously. 

Dependent: 
\begin{equation}
P(A and B) = P(A)P(B|A)
\end{equation}

Independent: 

\begin{equation}
P(A and B) = P(A)*P(B)
\end{equation}

---
# Independent Events

- $A$ and $B$ are independent if the occurrence of $A$ does not influence the occurrence of $B$, and if the occurrence of $B$ does not influence the occurrence of $A$. 

If two events $A$ and $B$ are independent, knowing that $A$ occurred does not inform the chances that $B$ occurred. We have:  

\begin{equation}
P(A|B) = P(A)
\end{equation}

\begin{equation}
P(B|A) = P(B)
\end{equation}
---
# M&Ms

```{r, echo=FALSE}
m<-data.frame(
  stringsAsFactors = FALSE,
             Color = c("Brown", "Red", "Yellow", "Green", "Orange", "Blue"),
             Count = c(13L, 13L, 14L, 16L, 20L, 24L)
)

m %>%
  flextable()
```
What is the *p*(blue and blue)?

```{r}

24/100*24/100

```
---
# Conditional Probablity

- The likelihood of an event or outcome occurring, based on the occurrence of a previous event or outcome

$$ P(A|B) = \frac{P(A\: \cap \: B)}{P(B)}  $$

Three probabilities 𝑝(A|B) is a conditional probability, while 𝑝(𝐴∩𝐵) is a joint probability and 𝑝(𝐵) is a marginal probability
---
# Conditional Probability

- Marginal probability: Likelihood that a randomly sampled outcome has 𝐴

- Conditional probability

  - Likelihood that an outcome randomly sampled from the subset with 𝐵 has 𝐴 (i.e., conditional is opposed to marginal)
  
  - We would say “𝐴 given 𝐵” or “𝐴 conditional on 𝐵” 
  
---
# Conditional Probablity

- Conditional probabilities allow us to update the probability of an event $A$ based on the occurrence of another event  

- $P(A)$ is called the _prior probability_  

- $P(A|B)$ is called the _posterior probability_  
---
# Multiplication rule  

\begin{equation}
P(A|B)P(B) = P(A\: \cap \: B)
\end{equation}
---
# Implication  

If $A$ and $B$ are independent events, we have: 
\begin{equation}
P(A\: \cap \: B) = P(A)P(B)
\end{equation}  

---
# Practice with grant proposal

You are about to send a grant proposal to an organization. While you read about the grant, you realize that your grant proposal will be sent to 5 different referees, who can be either social or cognitive psychologists. Imagine that for each grant proposal, the committee flips a coin five times and assigns the proposal to a social psychologist every time the flip returns heads, and to a cognitive psychologist every time the flip returns tails.  

Assume an infinite pool of social and cognitive psychologists. What are the chances that your grant proposal is assigned to 5 cognitive psychologists?   
---
# Practice with grant proposal

Let $C_i$ be the event that your grant proposal is assigned to a cognitive psychologist. Since the events are independent from each other, we have:

\small
$$
    \begin{split}
P(C_1 \: \cap C_2\: \cap C_3\: \cap C_4\: \cap C_5) &= P(C_1) \times P(C_2) \times P(C_3) \times P(C_4) \times P(C_5) \\
&= (\frac{1}{2})^5 \\
&= \frac{1}{32} \\
    \end{split}
$$
---
# Bayes' Rule  

- Reversing a conditional probability allows us to find $P(A|B)$ if we know $P(B|A)$: 

- Bayes' rule: 
\begin{equation}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{equation} 

```{r ltp, fig.show='hold', out.width="40%", echo = FALSE, fig.cap = ''}

knitr::include_graphics('images/Thomas_Bayes.gif')

```

---
# Monty Hall Problem

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/AD6eJlbFa2I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


---
# Monty Hall

```{r echo=FALSE,out.width="70%",fig.cap="",fig.show='hold',fig.align='center'}

knitr::include_graphics('images/monty.png')

```

- The winning strategy is to switch, but how is this possible?
  - Our intuition tells us our chance of winning the car increases from 1∕3 to 1∕2 when there are only two doors to choose from
  - In reality, our chance of winning the car remains 1∕3 if we stick with our original choice, but increases to 2∕3 if we switch

---
# Monty Hall Simulations

```{r}
set.seed(973)

monty <- function() {
    prize <- sample(x = 1:3, size = 1, replace = TRUE)
    choice <- sample(x = 1:3, size = 1, replace = TRUE)
    monty <- sample(x = c(1:3)[-c(choice, prize)], size = 1, replace = TRUE)
    return(ifelse(prize != choice, yes = "Switch", no = "Stick"))
}
monty()

```

---

```{r}
run <- rep(NA, 100000)

for (i in 1:100000) {
    run[i] <- monty()
}

prop.table(table(run))
## strategy
##   Stick  Switch 
## 0.33147 0.66853 

```

---
# Illustration: Covid-19  

If you tested positive for Covid-19, what is the chance you actually have it?

  1.  68% of the people who have Covid-19 test positive (sensitivity) 
  
  2.  99% of the people who do not have Covid-19 test negative  (specificity)
  
  3.  16%  of the population has Covid

---
# Illustration: Covid-19

- Let:   

  - $Covid$ be the event of having Covid-19   
  
  - $Pos$ be the event of testing positive 
  
  - $Neg$ be the event of testing negative  


- We know that:  

  - $P(Covid) = .16$    
  
  - $P(Pos|Covid) = .68$  
  
  - $P(Neg|Covid^C) = .99$   


- We need to calculate: 

  - $P(Covid|Pos)$     
---
# Illustration: Covid-19

- Bayes' Rule: 

\small
$$ P(Covid|Pos) = \frac{P(Pos|Covid)P(Covid)}{P(Pos)} $$

---
# Illustration: Covid-19  

$$
    \begin{split}
P(Pos) &= P(Pos \: \cap \: Covid) + P(Pos \: \cap \: Covid^C) \\
&= P(Pos|Covid)P(Covid) + P(Pos|Covid^C)P(Covid^C) \\ 
&= 0.85\times \frac{1}{130} + (1-0.96)\times (1-\frac{1}{130}) \\
&= 0.0462
    \end{split}
$$

We can now derive $P(Covid|Pos)$ using Bayes' rule:   

$$P(Covid|Pos) = \frac{.85 * \frac{1}{30}}{.0462} = .142$$
---
# Lessons from Bayes' rule 

- Based on the results of this test, the probability that your friend actually has covid-19 is .142   

  - That's a 14.2% chance of actually having covid-19.   
  
- Bayes' rule often yields counter-intuitive results!  

- Importance of base rates  
---
class: center middle

# Probability Theory vs. Statistical Inference 

---
# Probability Theory  

- We know the probability model of the random generative process    

- We ask: given the data generative process, what data is likely?  
---  
# Probability Theory  

- For any given random phenomenon, probability theory is a set of tools that
assume prior knowledge of:  

  - The sample space   
  
  - The probability of a set of events defined on that sample space   
  
- Allows you to find the probability of any other possible event from that sample space  

---
# Problem 

- We usually don't know the probability model  

- OK, we can find the probability of every outcome in the sample space by observing many many repetitions   

  - BUT most random phenomena cannot be repeated again, again, and again   

- We generally need to infer the probability of each possible outcome using information on a few realizations of the random phenomenon of interest     

---
# Probability and Statistics

- By knowing your population makeup, you have a better idea of the probability of obtaining certain samples.

- Probability links population with samples.

- Inferential statistics rely on this connection when they use sample data as the basis for making conclusions about populations.	

```{r, fig.align='center', echo=FALSE, warning=FALSE,  out.width = "50%"}

knitr::include_graphics("unnamed.png")
```
---
# Statisical Inference

- For any given random phenomenon, statistical inference is a set of tools that
uses knowledge of:  

  - A finite number of realizations of a random phenomenon   
  
- ... to tell you how to make educated guesses about:  

  - The sample space  
  
  - The probability of events defined on that sample space   
---
# Probability Theory vs. Statistical Inference 

- Probability theory asks:  

  - Assuming that the probability of observing heads in a coin flip is 0.5  
    - What is the probability of observing HHTT if we flip a coin four
times?  

- Statistical inference asks:   

  - Suppose that you flip a coin four times and observe HHTT  
  
     - What is your best guess for the probability of observing heads when flipping that coin? How confident are you in your guess?
  
---
class: middle center

#In-Class Analysis

---
# Data

- Florida voter registration data

```{r}
library(here)

voter=read.csv(here::here("static","slides","05-Probability","data", "florida-voters.csv"))

voter %>% 
  glimpse()

voter <- na.omit(voter) 

voter %>% 
  glimpse()

dim(voter)
```

---
# Data: Setup

```{r}

head(voter) %>%
  flextable::flextable()


```
---
# Marginal Probabilties

- What are these again?

- The probability of an event irrespective of the outcomes 

--

```{r}
library(knitr)
marg.race <- voter %>%
  count(race)%>%
  mutate(prop=prop.table(n)) %>%
  kable()

marg.race

```

---
# Gender


```{r}
marg.gender <- voter %>%
  group_by(gender) %>%
  summarise(n=n())%>%
  mutate(freq=n/sum(n)) %>%
  kable()

marg.gender

```
---
# Conditional Probability


$$ P(black|male) =  $$
```{r}
library(janitor)
cond_racegender <- voter %>%
  filter(gender=="m") %>%
  tabyl(race)%>%
  kable()

cond_racegender

```
---
# 

$$ P(black \cap male) $$
```{r}
library(janitor)

joint <- voter %>%
  select(race, gender) %>%
  group_by(race, gender) %>%
  count(race, gender) %>%
  ungroup() %>% 
  mutate(total=sum(n), prop=n/total)

joint
```

# Data: Independance
```{r}

marg.race <- voter %>%
 group_by(race)%>%
  tabyl(race)

marg.gender <- voter %>%
   group_by(gender)%>%
  tabyl(gender)

marg.race

marg.gender
```


```{r}

0.13*0.464 


```

```{r}
library(kableExtra)

joint <- voter %>%
  select(race, gender) %>%
  group_by(race, gender) %>%
  count(race, gender) %>%
  ungroup() %>% 
  mutate(total=sum(n), prop=n/total) %>%
  kable() %>%
  kable_styling() %>%
  row_spec(4, bold = T, background = "black")

joint
```

