---
title: "PSY 503: Foundations of Statistical Methods in Psychological Science"
subtitle: "Linear Modeling"
institute: "Princeton University"
author: "Jason Geller, Ph.D. (he/him/his)"
date:  'Updated:`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
      background-image: url("lover.png")
      background-size: cover
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "36%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  fig.show = TRUE,
  hiline = TRUE
)

hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)

style_solarized_dark(
  header_font_google = google_font("Work Sans"),
  header_h1_font_size = "36px",
  header_color = "black",
  text_font_google = google_font("Work Sans"),
  text_font_size = "28px",
  text_color = "black", 
  background_color = "white", 
  code_font_google = google_font("Share Tech Mono"),
  extra_css = list(
    ".remark-slide-content h2" = list(
      "margin-top" = "2em",
      "margin-bottom" = "2em"
    ),
    .big = list("font-size" = "150%"),
    .small = list("font-size" = "75%"),
    .subtle = list(opacity = "0.6"),
    ".countdown-has-style h3, .countdown-has-style h3 ~ p, .countdown-has-style h3 ~ ul" = list(
      "margin" = "0"
    ),
    ".countdown-has-style pre" = list(
      "margin-top" = "-10px"
    ),
    "p .remark-inline-code" = list(
      "background-color" = "white",
      "padding" = "2px 2px",
      "margin" = "0 -2px"
    ),
    blockquote = list("margin-left" = 0),
    "em" = list(color = "#2aa198")
  ),
)

```

```{r, echo=FALSE}
library(parameters)
library(tidyverse)
library(performance)
library(equatiomatic)
library(kableExtra)
library(broom)

# read in data
pitch=read_csv(here::here("static", "slides", "10-linear_modeling", "data", "age_pitch.csv"))
```
# Outline

- Correlation

- Regression (linear modeling)
---
# Correlation (*r*)

- Direction (positive or neagtive)

- Strength  (small, medium, or large)

$$r = \frac{covariance}{s_xs_y} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{(N - 1)s_x s_y}$$

---
# Correlations

```{r echo=FALSE,out.height="15%", out.width="70%",fig.cap="",fig.show='hold',fig.align='center'}

knitr::include_graphics('images/corr.png')
```
---
Example: 

# Simple Regression Example

- Pitch perception and age  N

  - What does the relationship look like?

```{r, fig.align='center', echo=FALSE, out.width="100%"}

# read in data
pitch=read_csv(here::here("static", "slides", "10-linear_modeling", "data", "age_pitch.csv"))

# Animation
# Change the point sizes manually
anim.1<- ggplot(pitch, aes(x=age, y=pitch))+
   geom_point()+
  theme(legend.position="top")

anim.1
```
---
# Statistical Test: Pearson's *r*

$$\textit{t}_r =  \frac{r\sqrt{N-2}}{\sqrt{1-r^2}}$$

```{r}
library(correlation) # easystats
library(see)

cor_result <- cor_test(pitch, "age", "pitch")
```


```{r, echo=FALSE, fig.align='center',out.width="100%"}
plot(cor_result,
  point = list(
    aes = list(color = "pitch", size = "age"),
    alpha = 0.66
  ),
  smooth = list(color = "black", se = FALSE)
) +
  see::theme_modern() +
  see::scale_color_material_c(palette = "rainbow", guide = "none") +
  scale_size_continuous(guide = "none")
```
---
# Nonparamteric Correlation

- Spearman’s rank correlation coefficient (ρ):

$$r_s = \frac{6 \sum d_i^2}{n(n^2 - 1)}$$

- It assesses how well the relationship between two variables can be described using a  monotonic (increasing or decreasing) function

- Rank order method
- range [-1,+1]

---
# Statistical Test: Spearman's *r*

```{r}
cor.test(pitch$pitch, pitch$age, data=pitch, method = "spearman")
```
---
# Effect Size Heuristics

- *r* < 0.1	very small
- 0.1  ≤ *r* < 0.3	small
- 0.3   ≤ *r* < 0.5	moderate
- *r* ≥ 0.5	large
---

# What is Linear Modeling?

- A way of describing a phenomenon

--

- A way of predicting the value of one variable from other variables

    - It is a hypothetical model of the relationship between two or more variables
    
    - The model used is a linear one, but doesn't always have to be! 
  
---
# Twitter Wars

```{r echo=FALSE,out.height="15%", out.width="75%",fig.cap="",fig.show='hold',fig.align='center'}

knitr::include_url('https://twitter.com/spyasin/status/1543542858493337615')

```

---
# Types of Linear Modeling

- **Simple Linear Regression**

    - One X variable (IV)

- **Multiple Linear Regression = MLR**

    - 2 or more X variables (IVs)
    
    - MLR types include: 

         - Simultaneous: Everything at once
         - Hierarchical: IVs in steps
         - Stepwise: Statistical regression 
---
# Describing a Straight Line

- We describe the relationship between variables using the equation of a straight line

$$Y_i = b_0 + b_1X_i + \varepsilon_i$$
---
# Describing a Straight Line

$$Y_i = b_0 + b_1X_i + \varepsilon_i$$

.pull-left[
- What is $b_i$?  

    - Regression coefficient for the predictor
    - Gradient (slope) of the regression line
    - Tells us how much we would expect y to change given a one-unit change in  x
    - Direction/Strength of Relationship
    
]

.pull-right[
- What is $b_0$?

    - Intercept (value of Y when X(s) = 0)
    - Point at which the regression line crosses the Y-axis
    
]
---
# The Best Fit Line and Least Squares

- The best fitting line is one that minimizes the squared difference between X and Y

- Ordinary least squares (OLS) is most common

  - What do squares have to do with it and why are they least sqaures?
---
# Example

.pull-left[
```{r, echo=FALSE, message=FALSE, fig.align='center', out.width="100%"}
library(ggplot2)
library(gganimate)
library(dplyr)

d <- mtcars
fit <- lm(mpg ~ hp, data = d)
d$predicted <- predict(fit)   # Save the predicted values
d$residuals <- residuals(fit) # Save the residual values

coefs<-coef(lm(mpg ~ hp, data = mtcars))

x<-d$hp
move_line<-c(seq(-6,6,.5),seq(6,-6,-.5))
total_error<-length(length(move_line))
cnt<-0
for(i in move_line){
  cnt<-cnt+1
  predicted_y <- coefs[2]*x + coefs[1]+i
  error_y <- (predicted_y-d$mpg)^2
  total_error[cnt]<-sqrt(sum(error_y)/32)
}

move_line_sims<-rep(move_line,each=32)
total_error_sims<-rep(total_error,each=32)
sims<-rep(1:50,each=32)

d<-d %>% slice(rep(row_number(), 50))

d<-cbind(d,sims,move_line_sims,total_error_sims)

anim<-ggplot(d, aes(x = hp, y = mpg)) +
  geom_abline(intercept = 30.09886+move_line_sims, slope = -0.06822828, aes(linetype='d'), color= 'red')+
  lims(x = c(0,400), y = c(-10,40))+
  geom_segment(aes(xend = hp, yend = predicted+move_line_sims, color="red"), alpha = .5) + 
  geom_point() +
  geom_rect(aes(ymin=predicted+move_line_sims, ymax=mpg, xmin=hp, xmax=hp+abs(predicted)+abs(residuals)+abs(move_line_sims), fill = total_error_sims), alpha = .2)+
  scale_fill_gradient(low="lightgrey", high="red")+
  geom_smooth(method = "lm", se = FALSE, color = "blue") + 
  theme_classic()+
  theme(legend.position="none")+
  xlab("X")+ylab("Y")+
  transition_manual(frames=sims)+
  enter_fade() + 
  exit_fade()+
  ease_aes('sine-in-out')

animate(anim,fps=5)
```
]

.pull-left[

- Shows two concepts:

  1. Regression line is "best fit line"
  
  2. The “best fit line” is the one that minimizes the sum of the squared deviations between each point and the line
]
---
# Visualizing Error

```{r, echo=FALSE, fig.align='center', out.width="100%"}

some_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),
                        X= c(3,5,4,2,6,7,8,9)) %>%
  mutate(Y_pred = predict.lm(lm(Y~X))) %>%
  mutate(Y_error = Y - Y_pred)

ggplot(some_data, aes(x=X, y=Y))+
  geom_point()+
  geom_smooth(method='lm', se=FALSE)+
  geom_point(aes(y=Y_pred), color='red') +
  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)
```
---
# Visualize Errors as Sqaures

.pull-left[

```{r,echo=TRUE}

some_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),
                        X= c(3,5,4,2,6,7,8,9)) %>%
  mutate(Y_pred = predict.lm(lm(Y~X))) %>%
  mutate(Y_error = Y - Y_pred)

g=ggplot(some_data, aes(x=X, y=Y))+
  geom_point()+
  geom_smooth(method='lm', se=FALSE)+
  geom_point(aes(y=Y_pred), color='red') +
  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+
  geom_rect(aes(ymin=Y, 
                ymax=Y_pred, 
                xmin=X,
                xmax=X+Y_error), 
            alpha = .2)
```

]

.pull-right[

```{r, fig.align='center', echo=FALSE, out.width="100%"}

some_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),
                        X= c(3,5,4,2,6,7,8,9)) %>%
  mutate(Y_pred = predict.lm(lm(Y~X))) %>%
  mutate(Y_error = Y - Y_pred)

ggplot(some_data, aes(x=X, y=Y))+
  geom_point()+
  geom_smooth(method='lm', se=FALSE)+
  geom_point(aes(y=Y_pred), color='red') +
  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+
  geom_rect(aes(ymin=Y, 
                ymax=Y_pred, 
                xmin=X,
                xmax=X+Y_error), 
            alpha = .2)+
  coord_cartesian(xlim=c(0,10),
                  ylim=c(0,10))
```

]
---
# Worse Fit Lines

```{r,echo=FALSE, out.width="100%"}
some_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),
                        X= c(3,5,4,2,6,7,8,9)) %>%
  mutate(Y_pred = predict.lm(lm(Y~X)))

some_data <- rbind(some_data,
                   some_data,
                   some_data,
                   some_data) %>%
  mutate(step = rep(1:4,each = 8),
         Y_pred = Y_pred + rep(c(.5,1,1.5,2), each = 8)) %>%
  mutate(Y_error = Y - Y_pred)

ggplot(some_data, aes(x=X, y=Y))+
  geom_smooth(method='lm', se=FALSE)+
  geom_point(aes(y=Y_pred), color='red') +
  geom_line(aes(x=X,y=Y_pred), color='red')+
  geom_point()+
  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+
  geom_rect(aes(ymin=Y, 
                ymax=Y_pred, 
                xmin=X,
                xmax=X+Y_error), 
            alpha = .2)+
  coord_cartesian(xlim=c(0,10),
                  ylim=c(0,10)) +
  facet_wrap(~step)
```
---
# Simple Regression Example

- A linear model fit to data with a numeric $X$ is classical regression

- Pitch perception and age 

  - As one ages, ability to hear certain pitch frequencies decreases

```{r, fig.align='center', echo=FALSE, out.width="100%"}

# Animation
# Change the point sizes manually
anim.1<- ggplot(pitch, aes(x=age, y=pitch))+
   geom_point()+
  theme(legend.position="top")

anim.1
```

---

```{r}

fit.pitch=lm(pitch~age, data=pitch)

```

```{r, fig.align='center', out.width="100%", echo=FALSE}

#Animation
# Change the point sizes manually
anim.1<- ggplot(pitch, aes(x=age, y=pitch))+
   geom_point()+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE) + 
  theme(legend.position="top")

anim.1
```

---
```{r, fig.align='center', echo=FALSE,out.width="100%"}

#Animation
# Change the point sizes manually
anim.1<- ggplot(pitch, aes(x=age, y=pitch))+
   geom_point()+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  theme(legend.position="top") + 
  annotate("text",x=60,y=190,label=(paste0("slope==",coef(lm(pitch$pitch~pitch$age))[2])),parse=TRUE)

anim.1
```
---
```{r, fig.align='center', echo=FALSE, out.width="100%"}
#Animation
# Change the point sizes manually
anim.1<- ggplot(pitch, aes(x=age, y=pitch))+
   geom_point()+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  theme(legend.position="top") + 
  annotate("text",x=60,y=190,label=(paste0("slope==",coef(lm(pitch$pitch~pitch$age))[2])),parse=TRUE)+
  geom_vline(xintercept=0, linetype="dotted") + 
  geom_point(x=0, y=216, colour="red", size=5, shape=4)

anim.1
```
---
$$ pitch = 216 + (-.7)*age$$

```{r, fig.align='center', echo=FALSE, out.width="100%"}
library(ggpmisc)
#Animation
# Change the point sizes manually
anim.1<- ggplot(pitch, aes(x=age, y=pitch))+
   geom_point()+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  theme(legend.position="top") + 
  annotate("text",x=60,y=190,label=(paste0("slope==",coef(lm(pitch$pitch~pitch$age))[2])),parse=TRUE)+
  geom_vline(xintercept=0, linetype="dotted") + 
  geom_point(x=0, y=216, colour="red", size=5, shape=4)

anim.1
```
---
$$ pitch = 216 + (-.7)*40$$
```{r, fig.align='center', echo=FALSE, out.width="100%"}

#Animation
# Change the point sizes manually
anim.1<- ggplot(pitch, aes(x=age, y=pitch))+
   geom_point()+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  theme(legend.position="top") + 
  annotate("text",x=60,y=190,label=(paste0("slope==",coef(lm(pitch$pitch~pitch$age))[2])),parse=TRUE)+
  geom_vline(xintercept=0, linetype="dotted") + 
  geom_point(x=0, y=216, colour="red", size=5, shape=4)

anim.1
```

---

$$ \hat{pitch} = 216 - 28$$

```{r, fig.align='center', echo=FALSE, out.width="100%"}

#Animation
# Change the point sizes manually
anim.1<- ggplot(pitch, aes(x=age, y=pitch))+
   geom_point()+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  theme(legend.position="top") + 
  annotate("text",x=60,y=190,label=(paste0("slope==",coef(lm(pitch$pitch~pitch$age))[2])),parse=TRUE)+
  geom_vline(xintercept=0, linetype="dotted") + 
  geom_point(x=0, y=216, colour="red", size=5, shape=4) + geom_point(x=40, y=188, colour="red", size=5, shape=4)

anim.1
```
---

# Residuals, Fitted Values, and Model Fit

- If we want to make inferences about the regression parameter estimates, then we also need an estimate of their variability. 

---
# SS Unexplained (Sums of Sqaures Error)

$$residual = y - \hat{y} = y - (x*\hat{\beta_x} + \hat{\beta_0})$$

$$SS_{error} = \sum_{i=1}^n{(y_i - \hat{y_i})^2} = \sum_{i=1}^n{residuals^2}$$

```{r, echo=FALSE, fig.align='center', out.width="100%"}
some_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),
                        X= c(3,5,4,2,6,7,8,9)) %>%
  mutate(Y_pred = predict.lm(lm(Y~X))) %>%
  mutate(Y_error = Y - Y_pred)

(res_plot <- ggplot(some_data, aes(x=X, y=Y))+
  geom_point()+
  geom_smooth(method='lm', se=FALSE)+
  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+
  geom_rect(aes(ymin=Y, 
                ymax=Y_pred, 
                xmin=X,
                xmax=X+Y_error), 
            alpha = .5,
            fill = "red")+
  coord_cartesian(xlim=c(0,10),
                  ylim=c(0,10))+
  theme_classic()+
  ggtitle("SS Unexplained (residual)")
  )

```

---
# SST 

> Squared differences between the observed dependent variable and its mean. 

$$SS_{total} = \sum{(Y_i - \bar{Y})^2}$$

```{r, echo=FALSE, out.width="100%"}

some_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),
                        X= c(3,5,4,2,6,7,8,9)) %>%
  mutate(Y_pred = mean(Y)) %>%
  mutate(Y_error = Y - Y_pred)

(total_plot <- ggplot(some_data, aes(x=X, y=Y))+
  geom_point()+
  geom_line(aes(y=Y_pred), color='black')+
  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+
  geom_rect(aes(ymin=Y, 
                ymax=Y_pred, 
                xmin=X,
                xmax=X+Y_error), 
            alpha = .2)+
  coord_cartesian(xlim=c(0,10),
                  ylim=c(0,10))+
  theme_classic()+
  ggtitle("SS Total")
  )

```


---
# Sums of Squares Regression (SS Explained)

> The sum of the differences between the predicted value and the mean of the dependent variable

$$SS_{Explained} = \sum (Y'_i - \bar{Y})^2$$

```{r, echo=FALSE, fig.align='center', out.width="100%"}

some_data <- data.frame(Y= c(1,2,4,3,5,4,6,5),
                        X= c(3,5,4,2,6,7,8,9)) %>%
  mutate(Y_pred = predict.lm(lm(Y~X)),
         Y_mean = mean(Y)) %>%
  mutate(Y_error = Y - Y_pred,
         Y_total = Y-Y_mean)

(exp_plot <- ggplot(some_data, aes(x=X, y=Y))+
  geom_point()+
  geom_line(aes(y=Y_mean), color='black')+
  geom_smooth(method='lm', se=FALSE)+
  geom_segment(aes(xend = X, y = Y_mean, yend = Y_pred), color='black')+
  geom_rect(aes(ymin=Y_mean, 
                ymax=Y_pred, 
               xmin=X,
                xmax=X+(Y_pred - Y_mean)), 
            alpha = .5,
            fill = "blue")+
  coord_cartesian(xlim=c(0,10),
                  ylim=c(0,10))+
  theme_classic()+
  ggtitle("SS Explained (by Regression of X)"))
```
---
# All Together

```{r, fig.align='center', out.width="100%"}

library(patchwork)

(total_plot +plot_spacer())/(exp_plot+res_plot)+
  plot_annotation(title = 'SStotal = SSexplained + SSunexplained')



```
---
# Statistical Tests

$$\begin{array}{c}
t_{N - p} = \frac{\hat{\beta} - \beta_{expected}}{SE_{\hat{\beta}}}\\
t_{N - p} = \frac{\hat{\beta} - 0}{SE_{\hat{\beta}}}\\
t_{N - p} = \frac{\hat{\beta} }{SE_{\hat{\beta}}}
\end{array}$$
---
# Standard Error

$$MS_{error} = \frac{SS_{error}}{df} = \frac{\sum_{i=1}^n{(y_i - \hat{y_i})^2} }{N - p}$$
$$SE_{model} = \sqrt{MS_{error}}SE_{model} = \sqrt{MS_{error}}$$

$$SE_{\hat{\beta}_x} = \frac{SE_{model}}{\sqrt{{\sum{(x_i - \bar{x})^2}}}}$$
---
# `broom` Regression 

tidy(): coefficent table
glance(): model summary
augment(): adds information about each observation

```{r}
library(broom)
library(report)

```
---

```{r}

tidy(fit.pitch)


```

---
```{r}

tidy(fit.pitch) %>%
  select(term, estimate)


```

```{r}
library(kableExtra)
d=augment(fit.pitch)# residuals and fitted values

head(d) %>%
  kbl() %>%
  kable_material_dark() 
```

```{r}
# get MSE
mse=performance_mse(fit.pitch)

SEM<-sqrt(mse)

ssr <- sum((pitch$age - mean(pitch$age))^2)

SE <- mse/sqrt(ssr)

```
---

# Effect Size: $R^2$

$$R^2 = 1 - \frac{SS_{\text{error}}}{SS_{\text{tot}}}$$
$$R^2 = 1 - \frac{SS_{unexplained}}{SS_{Total}} = \frac{SS_{explained}}{SS_{Total}}$$

- Standardized effect size

  - Amount of variance explained
  
  - Range: 0-1
  
- Take *r* and $r^2$

---
# $R^2$
```{r}
library(performance)

rq2=r2(fit.pitch)

rq2=rq2$R2


```

- $R^2$ of `r rq2` means `r rq2`% of variance is explained by Age


---
# Reporting

- "For each additional year, pitch dropped by 0.7 Hz (SE = 0.08)"

- “Age had a negative effect on pitch (estimate: -0.7, SE = 0.08, t = -8.4, p < 0.001).”

- `r report(fit.pitch)`
---
# Your turn

The dataset contains nutritional information on 77 Starbucks food items. Spend some time reading the help file of this dataset. For this problem, you will explore the relationship between the calories and carbohydrate grams in these items.

```{r}

starbucks <- read_csv("https://raw.githubusercontent.com/jgeller112/psy503-psych_stats/master/static/slides/10-linear_modeling/data/starbucks.csv")

```

1. Create a scatterplot of this data with calories on the x-axis and carbohydrate grams on the y-axis, and describe the relationship you see

2. In the scatterplot you made, what is the explanatory variable? What is the response variable? Why might you want to construct the problem in this way?

3. Fit a simple linear regression to this data, with carbohydrate grams as the dependent variable and the calories as the explanatory variable

4. Write the fitted model out using mathematical notation. Interpret the slope and the intercept parameters

5. Find and interpret the value of $R^2$ for this model


---
class: middle
# Multiple Predictors
---
# Multiple Regression Example

 $$Y_i = b_0 + b_1X_1i + b_2X_i + b_3X_3i+  \varepsilon_i$$

- Adding predictors to our linear equation


---
- Mental Health and Drug Use:
    
    - CESD = depression measure
    - PIL total = measure of meaning in life
    - AUDIT total = measure of alcohol use 
    - DAST total = measure of drug usage

```{r}
master <- haven::read_spss(here::here("static", "slides", "10-linear_modeling", "data", "regression_data.sav"))
master <- master[ , c(8:11)]
str(master)
```
---
# Regression Assumptions

- Missing
- Additive (more than one variable)
- Independence of residuals
- Linearity
- Normality of residuals
- Homogeneity of residual variance (“homoskedasticity”)
- Factors are not correlated with one another(multicollinearity) (more than one variable)

---
# Assumptions

- Missingness

```{r}
summary(master)
nomiss <- na.omit(master)
nrow(master)
nrow(nomiss)
```
---
# Example: Outliers 

- A few new outlier checks:

    - Mahalanobis
    - Leverage scores
    - Cook's distance

- Because we are using regression as our model, we may consider using multiple checks before excluding outliers. 
---
# Example: Mahalanobis 

- The `mahalanobis()` function we have used previously.
- Since we are going to use multiple criteria, we are going to save if they are an outlier or not

- The table tells us: 0 (not outliers) and 1 (considered an outlier) for just Mahalanobis values

```{r echo=TRUE, message=FALSE, eval=TRUE, warning=FALSE}
mahal <- mahalanobis(nomiss, 
                    colMeans(nomiss), 
                    cov(nomiss))
cutmahal <- qchisq(1-.001, ncol(nomiss))
badmahal <- as.numeric(mahal > cutmahal) ##note the direction of the > 
table(badmahal)
```
---
# Example: Other Outliers

- To get the other outlier statistics, we have to use the regression model we wish to test. - We will use the `lm()` function with our regression formula.
- `Y ~ X + X`: Y is approximated by X plus X.
- So we will predict depression scores (CESD) with meaning, drugs, and alcohol.

```{r echo=TRUE, message=FALSE, warning=FALSE}
model1 <- lm(CESD_total ~ PIL_total + AUDIT_TOTAL_NEW + DAST_TOTAL_NEW,
             data = nomiss)
```

---
# Example: Leverage 

- **Definition** - influence of that data point on the slope

- Each score is the change in slope if you exclude that data point

- How do we calculate how much change is bad?

    - $\frac{2K+2}{N}$
    - K is the number of predictors
    - N is the sample size 
---
#  Example: Leverage 

```{r echo=TRUE, message=FALSE, warning=FALSE}
k <- 3 ##number of IVs
leverage <- hatvalues(fit.pitch)
cutleverage <- (2*k+2) / nrow(pitch)
badleverage <- as.numeric(leverage > cutleverage)
table(badleverage)
```
---
# Example: Cook's Distance 

- Influence **(Cook's Distance)** - a measure of how much of an effect that single case has on the whole model 
- Often described as leverage + discrepancy 

- How do we calculate how much change is bad?
  
    - $\frac{4}{N-K-1}$

```{r echo=TRUE, message=FALSE, warning=FALSE}
cooks <- cooks.distance(fit.pitch)
cutcooks <- 4 / (nrow(pitch) - k - 1)
badcooks <- as.numeric(cooks > cutcooks)
table(badcooks)
```
---
# Example: Outliers Combined

- What do I do with all these numbers?

- Create a total score for the number of indicators a data point has

- You can decide what rule to use, but a suggestion is 2 or more indicators is an outliers

```{r echo=TRUE, message=FALSE, warning=FALSE}
##add them up!
totalout <- badmahal + badleverage + badcooks
table(totalout)
noout <- subset(nomiss, totalout < 2)
```
---
# Assumptions

- Now that we got rid of outliers, we need to run that model again, without the outliers.

```{r echo=TRUE, message=FALSE, warning=FALSE}
model2 <- lm(CESD_total ~ PIL_total + AUDIT_TOTAL_NEW + DAST_TOTAL_NEW, 
             data = noout)
```
---
# Assumptions

  - Additivity
  
    -  The effects of different independent variables on the expected value of the dependent variable are additive.

  
    
```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(model2, correlation = TRUE)
```


---
# Assumptions

- Linearity

  - Check (use a plot) that your variables are linearly related
  
```{r, echo=FALSE, fig.align='center', out.width="100%"}
library(GGally)

ggpairs(master)
```
--- 
# Assumptions

- Normality

  - **Applies to residuals and not the distribution of the data**
```{r echo=TRUE, message=FALSE, warning=FALSE}
model2 <- lm(CESD_total ~ PIL_total + AUDIT_TOTAL_NEW + DAST_TOTAL_NEW, 
             data = noout)

check_normality(model2)
```

---
# Assumptions

- Homogeneity & Homoscedasticity

```{r, fig.align='center', out.width="80%"}
knitr::include_graphics("images/homohetero.svg")
```


---
# Assumptions

- Homogeneity & Homoscedasticity

```{r, eval=FALSE}
#easystats

performance::check_homogeneity(model2)

```

---
# Multicolinearity: 

- You want X and Y to be correlated
- You do not want the Xs to be highly correlated 

  - Causes estimates to be untrustworthy

    - "Bouncing betas"
  
- Test: 

  - VIF (variance inflation factor)
  
    - Rule of thumb:
    
      -  > 10 problem with multicolinearity
---
# Assumptions

- Multicolinearity

```{r}
#easystats performance package
check_collinearity(model2)
```

---
# Check Assumptions

```{r}
check_normality(model2)
check_heteroscedasticity(model2)
check_autocorrelation(model2)
check_collinearity(model2)
```
---
# Check Assumptions

```{r, fig.align='center', out.width="100%"}
check_model(model2)
```

---
# Example: Assumption Alternatives

- If your assumptions go wrong:
    - Linearity - try nonlinear regression or nonparametric regression
    - Normality - more subjects, still fairly robust
    
    - Homogeneity/Homoscedasticity - bootstrapping
    
    - Multicolineary - drop one of the predictors; ridge regression
---

# Regression Model

- Is my overall model *(i.e., the regression equation)* useful at predicting the outcome variable?

    - Use the model summary, F-test, and $R^2$

- How useful are each of the individual predictors for my model?

    - Use the coefficients, t-test, and $pr^2$
---
# Overall Model

 Our overall model uses an *F*-test
 
- However, we can think about the hypotheses for the overall test being:

    - H0: We cannot predict the dependent variable
    
    - H1: We can predict the dependent variable
    
- Generally, this form does not include two tailed tests because the math is squared, so it is impossible to get negative values in the statistical test

---
# F-Statistic, Explained Over Unexplained

- F-statistics use measures of variance, which are sums of squares divided by relevant degrees of freedom

$$F = \frac{SS_{Explained}/df1}{SS_{Unexplained}/df2} = \frac{MS_{Explained}}{MS_{Unexplained}}$$

- If explained = unexplained, then F=1

- If explained > then, F >1 

- If explained < unexplained, F < 1

---
# ANOVA

```{r}

anova(lm(pitch~age, data=pitch))

```
---

# Individual Predictors

- We test the individual predictors with a t-test:
  
    - $t = \frac{b}{SE}$
    - Therefore, the model for each individual predictor is our coefficient b. 
    - Single sample t-test to determine if the b value is different from zero
    
---
# Individual Predictors: Standardization 

- b = unstandardized regression coefficient

    - For every one unit increase in X, there will be b units increase in Y.

- $\beta$ = standardized regression coefficient

    - b in standard deviation units
    - For every one SD increase in X, there will be $\beta$ SDs increase in Y
    
- b or $\beta$?:

    - b is more interpretable given your specific problem
    
    - $\beta$ is more interpretable given differences in scales for different variables
---

## Example: Overall Model

- Is the overall model significant? Yes!

```{r}
summary(model2)
library(papaja)
apa_style <- apa_print(model2)
apa_style$full_result$modelfit
```

- `r apa_style$full_result$modelfit`

## Example: Predictors

```{r}
summary(model2)
```

## Example: Predictors

```{r}
apa_style$full_result$PIL_total
apa_style$full_result$AUDIT_TOTAL_NEW
apa_style$full_result$DAST_TOTAL_NEW
```

- Meaning: `r apa_style$full_result$PIL_total`
- Alcohol: `r apa_style$full_result$AUDIT_TOTAL_NEW`
- Drugs: `r apa_style$full_result$DAST_TOTAL_NEW`

## Example: Predictors

- Two concerns:

    - What if I wanted to use beta because these are very different scales?
    - What about an effect size for each individual predictor?
---
# Beta

- You can use the `QuantPsyc` package for $\beta$ values. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(QuantPsyc)
lm.beta(model2)
```

```{r}
#easystats
standardize_parameters(model2)
```
---
# Effect Size 

- R is the multiple correlation
- All overlap in Y, used for overall model
- $A+B+C/(A+B+C+D)$

```{r, echo = FALSE, out.width="50%", fig.align='center'}
knitr::include_graphics("https://raw.githubusercontent.com/doomlab/learnSTATS/master/vignettes/pictures/regression/19.PNG")
```

## Example: Effect Size 

- sr is the semi-partial correlations 
- Unique contribution of IV to $R^2$ for those IVs

- Increase in proportion of explained Y variance when X is added to the equation

- $A/(A+B+C+D)$

```{r, echo = FALSE, out.width="50%", fig.align='center'}
knitr::include_graphics("https://raw.githubusercontent.com/doomlab/learnSTATS/master/vignettes/pictures/regression/19.PNG")
```
---
## Example: Effect Size 

- pr is the partial correlation

- Proportion in variance in Y not explained by other predictors but this X only

- $A/(A+D)$
- Pr > sr

```{r, echo = FALSE, out.width="50%", fig.align='center'}
knitr::include_url("https://github.com/doomlab/learnSTATS/blob/master/vignettes/pictures/regression/19.PNG")
```
--
# Partial Correlations

- We would add these to our other reports:

    - Meaning: `r apa_style$full_result$PIL_total`, $pr^2 = .30$
    - Alcohol: `r apa_style$full_result$AUDIT_TOTAL_NEW`, $pr^2 < .01$
    - Drugs: `r apa_style$full_result$DAST_TOTAL_NEW`, $pr^2 < .01$

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(ppcor)
partials <- pcor(noout)
partials$estimate^2
```
