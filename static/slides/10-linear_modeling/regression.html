<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>PSY 503: Foundations of Statistical Methods in Psychological Science</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jason Geller, Ph.D. (he/him/his)" />
    <script src="regression_files/header-attrs/header-attrs.js"></script>
    <script src="regression_files/kePrint/kePrint.js"></script>
    <link href="regression_files/lightable/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# PSY 503: Foundations of Statistical Methods in Psychological Science
]
.subtitle[
## Linear Modeling
]
.author[
### Jason Geller, Ph.D. (he/him/his)
]
.institute[
### Princeton University
]
.date[
### Updated:2022-10-18
]

---







# Outline

- Correlation

- Regression (linear modeling)
---
# Correlation (*r*)

- Direction (positive or neagtive)

- Strength  (small, medium, or large)

`$$r = \frac{covariance}{s_xs_y} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{(N - 1)s_x s_y}$$`

---
# Correlations

&lt;img src="images/corr.png" width="70%" height="15%" style="display: block; margin: auto;" /&gt;
---
Example: 

# Simple Regression Example

- Pitch perception and age  N

  - What does the relationship look like?

&lt;img src="regression_files/figure-html/unnamed-chunk-3-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
# Statistical Test: Pearson's *r*

`$$\textit{t}_r =  \frac{r\sqrt{N-2}}{\sqrt{1-r^2}}$$`


```r
library(correlation) # easystats
library(see)

cor_result &lt;- cor_test(pitch, "age", "pitch")
```


&lt;img src="regression_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
# Nonparamteric Correlation

- Spearman’s rank correlation coefficient (ρ):

`$$r_s = \frac{6 \sum d_i^2}{n(n^2 - 1)}$$`

- It assesses how well the relationship between two variables can be described using a  monotonic (increasing or decreasing) function

- Rank order method
- range [-1,+1]

---
# Statistical Test: Spearman's *r*


```r
cor.test(pitch$pitch, pitch$age, data=pitch, method = "spearman")
```

```
## 
## 	Spearman's rank correlation rho
## 
## data:  pitch$pitch and pitch$age
## S = 68527, p-value = 0.0003087
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##       rho 
## -0.430486
```
---
# Effect Size Heuristics

- *r* &lt; 0.1	very small
- 0.1  ≤ *r* &lt; 0.3	small
- 0.3   ≤ *r* &lt; 0.5	moderate
- *r* ≥ 0.5	large
---

# What is Linear Modeling?

- A way of describing a phenomenon

--

- A way of predicting the value of one variable from other variables

    - It is a hypothetical model of the relationship between two or more variables
    
    - The model used is a linear one, but doesn't always have to be! 
  
---
# Twitter Wars

&lt;iframe src="https://twitter.com/spyasin/status/1543542858493337615" width="75%" height="400px" data-external="1"&gt;&lt;/iframe&gt;

---
# Types of Linear Modeling

- **Simple Linear Regression**

    - One X variable (IV)

- **Multiple Linear Regression = MLR**

    - 2 or more X variables (IVs)
    
    - MLR types include: 

         - Simultaneous: Everything at once
         - Hierarchical: IVs in steps
         - Stepwise: Statistical regression 
---
# Describing a Straight Line

- We describe the relationship between variables using the equation of a straight line

`$$Y_i = b_0 + b_1X_i + \varepsilon_i$$`
---
# Describing a Straight Line

`$$Y_i = b_0 + b_1X_i + \varepsilon_i$$`

.pull-left[
- What is `\(b_i\)`?  

    - Regression coefficient for the predictor
    - Gradient (slope) of the regression line
    - Tells us how much we would expect y to change given a one-unit change in  x
    - Direction/Strength of Relationship
    
]

.pull-right[
- What is `\(b_0\)`?

    - Intercept (value of Y when X(s) = 0)
    - Point at which the regression line crosses the Y-axis
    
]
---
# The Best Fit Line and Least Squares

- The best fitting line is one that minimizes the squared difference between X and Y

- Ordinary least squares (OLS) is most common

  - What do squares have to do with it and why are they least sqaures?
---
# Example

.pull-left[
&lt;img src="regression_files/figure-html/unnamed-chunk-8-1.gif" width="100%" style="display: block; margin: auto;" /&gt;
]

.pull-left[

- Shows two concepts:

  1. Regression line is "best fit line"
  
  2. The “best fit line” is the one that minimizes the sum of the squared deviations between each point and the line
]
---
# Visualizing Error

&lt;img src="regression_files/figure-html/unnamed-chunk-9-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
# Visualize Errors as Sqaures

.pull-left[


```r
some_data &lt;- data.frame(Y= c(1,2,4,3,5,4,6,5),
                        X= c(3,5,4,2,6,7,8,9)) %&gt;%
  mutate(Y_pred = predict.lm(lm(Y~X))) %&gt;%
  mutate(Y_error = Y - Y_pred)

g=ggplot(some_data, aes(x=X, y=Y))+
  geom_point()+
  geom_smooth(method='lm', se=FALSE)+
  geom_point(aes(y=Y_pred), color='red') +
  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+
  geom_rect(aes(ymin=Y, 
                ymax=Y_pred, 
                xmin=X,
                xmax=X+Y_error), 
            alpha = .2)
```

]

.pull-right[

&lt;img src="regression_files/figure-html/unnamed-chunk-11-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]
---
# Worse Fit Lines

&lt;img src="regression_files/figure-html/unnamed-chunk-12-1.png" width="100%" /&gt;
---
# Simple Regression Example

- A linear model fit to data with a numeric `\(X\)` is classical regression

- Pitch perception and age 

  - As one ages, ability to hear certain pitch frequencies decreases

&lt;img src="regression_files/figure-html/unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---


```r
fit.pitch=lm(pitch~age, data=pitch)
```

&lt;img src="regression_files/figure-html/unnamed-chunk-15-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
&lt;img src="regression_files/figure-html/unnamed-chunk-16-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
&lt;img src="regression_files/figure-html/unnamed-chunk-17-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
$$ pitch = 216 + (-.7)*age$$

&lt;img src="regression_files/figure-html/unnamed-chunk-18-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
$$ pitch = 216 + (-.7)*40$$
&lt;img src="regression_files/figure-html/unnamed-chunk-19-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

$$ \hat{pitch} = 216 - 28$$

&lt;img src="regression_files/figure-html/unnamed-chunk-20-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---

# Residuals, Fitted Values, and Model Fit

- If we want to make inferences about the regression parameter estimates, then we also need an estimate of their variability. 

---
# SS Unexplained (Sums of Sqaures Error)

`$$residual = y - \hat{y} = y - (x*\hat{\beta_x} + \hat{\beta_0})$$`

`$$SS_{error} = \sum_{i=1}^n{(y_i - \hat{y_i})^2} = \sum_{i=1}^n{residuals^2}$$`

&lt;img src="regression_files/figure-html/unnamed-chunk-21-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# SST 

&gt; Squared differences between the observed dependent variable and its mean. 

`$$SS_{total} = \sum{(Y_i - \bar{Y})^2}$$`

&lt;img src="regression_files/figure-html/unnamed-chunk-22-1.png" width="100%" /&gt;


---
# Sums of Squares Regression (SS Explained)

&gt; The sum of the differences between the predicted value and the mean of the dependent variable

`$$SS_{Explained} = \sum (Y'_i - \bar{Y})^2$$`

&lt;img src="regression_files/figure-html/unnamed-chunk-23-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
# All Together


```r
library(patchwork)

(total_plot +plot_spacer())/(exp_plot+res_plot)+
  plot_annotation(title = 'SStotal = SSexplained + SSunexplained')
```

&lt;img src="regression_files/figure-html/unnamed-chunk-24-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
# Statistical Tests

`$$\begin{array}{c}
t_{N - p} = \frac{\hat{\beta} - \beta_{expected}}{SE_{\hat{\beta}}}\\
t_{N - p} = \frac{\hat{\beta} - 0}{SE_{\hat{\beta}}}\\
t_{N - p} = \frac{\hat{\beta} }{SE_{\hat{\beta}}}
\end{array}$$`
---
# Standard Error

`$$MS_{error} = \frac{SS_{error}}{df} = \frac{\sum_{i=1}^n{(y_i - \hat{y_i})^2} }{N - p}$$`
`$$SE_{model} = \sqrt{MS_{error}}SE_{model} = \sqrt{MS_{error}}$$`

`$$SE_{\hat{\beta}_x} = \frac{SE_{model}}{\sqrt{{\sum{(x_i - \bar{x})^2}}}}$$`
---
# `broom` Regression 

tidy(): coefficent table
glance(): model summary
augment(): adds information about each observation


```r
library(broom)
library(report)
```
---


```r
tidy(fit.pitch)
```

```
## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)  216.       4.95       43.6  2.49e-49
## 2 age           -0.694    0.0822     -8.44 5.50e-12
```

---

```r
tidy(fit.pitch) %&gt;%
  select(term, estimate)
```

```
## # A tibble: 2 × 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)  216.   
## 2 age           -0.694
```


```r
library(kableExtra)
d=augment(fit.pitch)# residuals and fitted values

head(d) %&gt;%
  kbl() %&gt;%
  kable_material_dark() 
```

&lt;table class=" lightable-material-dark" style='font-family: "Source Sans Pro", helvetica, sans-serif; margin-left: auto; margin-right: auto;'&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; pitch &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; age &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; .fitted &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; .resid &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; .hat &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; .sigma &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; .cooksd &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; .std.resid &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 165.8599 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 66.41 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 169.8942 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.034248 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0202980 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.839121 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0022301 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.4639750 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 169.5648 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 64.35 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 171.3231 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.758256 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0179033 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.851203 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0003718 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.2019687 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 171.8329 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 61.89 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 173.0294 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.196494 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0160178 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.852721 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0001535 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.1373081 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 170.9136 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 54.58 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 178.0998 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -7.186207 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0166716 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.806824 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0057691 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.8249534 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 155.6435 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 69.41 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 167.8133 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -12.169830 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0251153 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.716784 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0253588 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.4030943 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 168.9725 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 61.98 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 172.9670 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -3.994467 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0160681 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.839477 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0017159 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.4584113 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


```r
# get MSE
mse=performance_mse(fit.pitch)

SEM&lt;-sqrt(mse)

ssr &lt;- sum((pitch$age - mean(pitch$age))^2)

SE &lt;- mse/sqrt(ssr)
```
---

# Effect Size: `\(R^2\)`

`$$R^2 = 1 - \frac{SS_{\text{error}}}{SS_{\text{tot}}}$$`
`$$R^2 = 1 - \frac{SS_{unexplained}}{SS_{Total}} = \frac{SS_{explained}}{SS_{Total}}$$`

- Standardized effect size

  - Amount of variance explained
  
  - Range: 0-1
  
- Take *r* and `\(r^2\)`

---
# `\(R^2\)`

```r
library(performance)

rq2=r2(fit.pitch)

rq2=rq2$R2
```

- `\(R^2\)` of 0.5265089 means 0.5265089% of variance is explained by Age


---
# Reporting

- "For each additional year, pitch dropped by 0.7 Hz (SE = 0.08)"

- “Age had a negative effect on pitch (estimate: -0.7, SE = 0.08, t = -8.4, p &lt; 0.001).”

- We fitted a linear model (estimated using OLS) to predict pitch with age (formula: pitch ~ age). The model explains a statistically significant and substantial proportion of variance (R2 = 0.53, F(1, 64) = 71.17, p &lt; .001, adj. R2 = 0.52). The model's intercept, corresponding to age = 0, is at 215.96 (95% CI [206.07, 225.85], t(64) = 43.63, p &lt; .001). Within this model:

  - The effect of age is statistically significant and negative (beta = -0.69, 95% CI [-0.86, -0.53], t(64) = -8.44, p &lt; .001; Std. beta = -0.73, 95% CI [-0.90, -0.55])

Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using a Wald t-distribution approximation.
---
# Your turn

The dataset contains nutritional information on 77 Starbucks food items. Spend some time reading the help file of this dataset. For this problem, you will explore the relationship between the calories and carbohydrate grams in these items.


```r
starbucks &lt;- read_csv("https://raw.githubusercontent.com/jgeller112/psy503-psych_stats/master/static/slides/10-linear_modeling/data/starbucks.csv")
```

1. Create a scatterplot of this data with calories on the x-axis and carbohydrate grams on the y-axis, and describe the relationship you see

2. In the scatterplot you made, what is the explanatory variable? What is the response variable? Why might you want to construct the problem in this way?

3. Fit a simple linear regression to this data, with carbohydrate grams as the dependent variable and the calories as the explanatory variable

4. Write the fitted model out using mathematical notation. Interpret the slope and the intercept parameters

5. Find and interpret the value of `\(R^2\)` for this model


---
class: middle
# Multiple Predictors
---
# Multiple Regression Example

 `$$Y_i = b_0 + b_1X_1i + b_2X_i + b_3X_3i+  \varepsilon_i$$`

- Adding predictors to our linear equation


---
- Mental Health and Drug Use:
    
    - CESD = depression measure
    - PIL total = measure of meaning in life
    - AUDIT total = measure of alcohol use 
    - DAST total = measure of drug usage


```r
master &lt;- haven::read_spss(here::here("static", "slides", "10-linear_modeling", "data", "regression_data.sav"))
master &lt;- master[ , c(8:11)]
str(master)
```

```
## tibble [267 × 4] (S3: tbl_df/tbl/data.frame)
##  $ PIL_total      : num [1:267] 121 76 98 122 99 134 102 124 126 112 ...
##   ..- attr(*, "format.spss")= chr "F8.2"
##   ..- attr(*, "display_width")= int 11
##  $ CESD_total     : num [1:267] 28 37 20 15 7 7 27 10 9 8 ...
##   ..- attr(*, "format.spss")= chr "F8.2"
##   ..- attr(*, "display_width")= int 12
##  $ AUDIT_TOTAL_NEW: num [1:267] 1 5 3 3 2 3 2 1 1 7 ...
##   ..- attr(*, "format.spss")= chr "F8.2"
##   ..- attr(*, "display_width")= int 17
##  $ DAST_TOTAL_NEW : num [1:267] 0 0 0 1 0 0 1 0 0 1 ...
##   ..- attr(*, "format.spss")= chr "F8.2"
##   ..- attr(*, "display_width")= int 16
```
---
# Regression Assumptions

- Missing
- Additive (more than one variable)
- Independence of residuals
- Linearity
- Normality of residuals
- Homogeneity of residual variance (“homoskedasticity”)
- Factors are not correlated with one another(multicollinearity) (more than one variable)

---
# Assumptions

- Missingness


```r
summary(master)
```

```
##    PIL_total       CESD_total   AUDIT_TOTAL_NEW  DAST_TOTAL_NEW 
##  Min.   : 60.0   Min.   : 0.0   Min.   : 0.000   Min.   :0.000  
##  1st Qu.:103.0   1st Qu.: 7.0   1st Qu.: 2.000   1st Qu.:0.000  
##  Median :111.0   Median :11.0   Median : 5.000   Median :0.000  
##  Mean   :110.7   Mean   :13.2   Mean   : 6.807   Mean   :0.906  
##  3rd Qu.:121.0   3rd Qu.:17.0   3rd Qu.:11.000   3rd Qu.:1.000  
##  Max.   :138.0   Max.   :47.0   Max.   :31.000   Max.   :9.000  
##                                                  NA's   :1
```

```r
nomiss &lt;- na.omit(master)
nrow(master)
```

```
## [1] 267
```

```r
nrow(nomiss)
```

```
## [1] 266
```
---
# Example: Outliers 

- A few new outlier checks:

    - Mahalanobis
    - Leverage scores
    - Cook's distance

- Because we are using regression as our model, we may consider using multiple checks before excluding outliers. 
---
# Example: Mahalanobis 

- The `mahalanobis()` function we have used previously.
- Since we are going to use multiple criteria, we are going to save if they are an outlier or not

- The table tells us: 0 (not outliers) and 1 (considered an outlier) for just Mahalanobis values


```r
mahal &lt;- mahalanobis(nomiss, 
                    colMeans(nomiss), 
                    cov(nomiss))
cutmahal &lt;- qchisq(1-.001, ncol(nomiss))
badmahal &lt;- as.numeric(mahal &gt; cutmahal) ##note the direction of the &gt; 
table(badmahal)
```

```
## badmahal
##   0   1 
## 261   5
```
---
# Example: Other Outliers

- To get the other outlier statistics, we have to use the regression model we wish to test. - We will use the `lm()` function with our regression formula.
- `Y ~ X + X`: Y is approximated by X plus X.
- So we will predict depression scores (CESD) with meaning, drugs, and alcohol.


```r
model1 &lt;- lm(CESD_total ~ PIL_total + AUDIT_TOTAL_NEW + DAST_TOTAL_NEW,
             data = nomiss)
```

---
# Example: Leverage 

- **Definition** - influence of that data point on the slope

- Each score is the change in slope if you exclude that data point

- How do we calculate how much change is bad?

    - `\(\frac{2K+2}{N}\)`
    - K is the number of predictors
    - N is the sample size 
---
#  Example: Leverage 


```r
k &lt;- 3 ##number of IVs
leverage &lt;- hatvalues(fit.pitch)
cutleverage &lt;- (2*k+2) / nrow(pitch)
badleverage &lt;- as.numeric(leverage &gt; cutleverage)
table(badleverage)
```

```
## badleverage
##  0  1 
## 63  3
```
---
# Example: Cook's Distance 

- Influence **(Cook's Distance)** - a measure of how much of an effect that single case has on the whole model 
- Often described as leverage + discrepancy 

- How do we calculate how much change is bad?
  
    - `\(\frac{4}{N-K-1}\)`


```r
cooks &lt;- cooks.distance(fit.pitch)
cutcooks &lt;- 4 / (nrow(pitch) - k - 1)
badcooks &lt;- as.numeric(cooks &gt; cutcooks)
table(badcooks)
```

```
## badcooks
##  0  1 
## 60  6
```
---
# Example: Outliers Combined

- What do I do with all these numbers?

- Create a total score for the number of indicators a data point has

- You can decide what rule to use, but a suggestion is 2 or more indicators is an outliers


```r
##add them up!
totalout &lt;- badmahal + badleverage + badcooks
table(totalout)
```

```
## totalout
##   0   1   2 
## 229  33   4
```

```r
noout &lt;- subset(nomiss, totalout &lt; 2)
```
---
# Assumptions

- Now that we got rid of outliers, we need to run that model again, without the outliers.


```r
model2 &lt;- lm(CESD_total ~ PIL_total + AUDIT_TOTAL_NEW + DAST_TOTAL_NEW, 
             data = noout)
```
---
# Assumptions

  - Additivity
  
    -  The effects of different independent variables on the expected value of the dependent variable are additive.

  
    

```r
summary(model2, correlation = TRUE)
```

```
## 
## Call:
## lm(formula = CESD_total ~ PIL_total + AUDIT_TOTAL_NEW + DAST_TOTAL_NEW, 
##     data = noout)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -19.857  -5.230  -1.231   3.540  28.724 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     56.25004    3.92276  14.339   &lt;2e-16 ***
## PIL_total       -0.38990    0.03406 -11.446   &lt;2e-16 ***
## AUDIT_TOTAL_NEW -0.09947    0.09437  -1.054   0.2928    
## DAST_TOTAL_NEW   1.02970    0.40030   2.572   0.0107 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 7.578 on 258 degrees of freedom
## Multiple R-squared:  0.3676,	Adjusted R-squared:  0.3602 
## F-statistic: 49.98 on 3 and 258 DF,  p-value: &lt; 2.2e-16
## 
## Correlation of Coefficients:
##                 (Intercept) PIL_total AUDIT_TOTAL_NEW
## PIL_total       -0.98                                
## AUDIT_TOTAL_NEW -0.17        0.06                    
## DAST_TOTAL_NEW  -0.12        0.11     -0.51
```


---
# Assumptions

- Linearity

  - Check (use a plot) that your variables are linearly related
  
&lt;img src="regression_files/figure-html/unnamed-chunk-41-1.png" width="100%" style="display: block; margin: auto;" /&gt;
--- 
# Assumptions

- Normality

  - **Applies to residuals and not the distribution of the data**

```r
model2 &lt;- lm(CESD_total ~ PIL_total + AUDIT_TOTAL_NEW + DAST_TOTAL_NEW, 
             data = noout)

check_normality(model2)
```

```
## Warning: Non-normality of residuals detected (p &lt; .001).
```

---
# Assumptions

- Homogeneity &amp; Homoscedasticity


```r
knitr::include_graphics("images/homohetero.svg")
```

&lt;img src="images/homohetero.svg" width="80%" style="display: block; margin: auto;" /&gt;


---
# Assumptions

- Homogeneity &amp; Homoscedasticity


```r
#easystats

performance::check_homogeneity(model2)
```

---
# Multicolinearity: 

- You want X and Y to be correlated
- You do not want the Xs to be highly correlated 

  - Causes estimates to be untrustworthy

    - "Bouncing betas"
  
- Test: 

  - VIF (variance inflation factor)
  
    - Rule of thumb:
    
      -  &gt; 10 problem with multicolinearity
---
# Assumptions

- Multicolinearity


```r
#easystats performance package
check_collinearity(model2)
```

```
## # Check for Multicollinearity
## 
## Low Correlation
## 
##             Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI
##        PIL_total 1.03 [1.00, 2.59]         1.02      0.97     [0.39, 1.00]
##  AUDIT_TOTAL_NEW 1.38 [1.22, 1.66]         1.17      0.73     [0.60, 0.82]
##   DAST_TOTAL_NEW 1.39 [1.23, 1.67]         1.18      0.72     [0.60, 0.81]
```

---
# Check Assumptions


```r
check_normality(model2)
```

```
## Warning: Non-normality of residuals detected (p &lt; .001).
```

```r
check_heteroscedasticity(model2)
```

```
## Warning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).
```

```r
check_autocorrelation(model2)
```

```
## Warning: Autocorrelated residuals detected (p &lt; .001).
```

```r
check_collinearity(model2)
```

```
## # Check for Multicollinearity
## 
## Low Correlation
## 
##             Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI
##        PIL_total 1.03 [1.00, 2.59]         1.02      0.97     [0.39, 1.00]
##  AUDIT_TOTAL_NEW 1.38 [1.22, 1.66]         1.17      0.73     [0.60, 0.82]
##   DAST_TOTAL_NEW 1.39 [1.23, 1.67]         1.18      0.72     [0.60, 0.81]
```
---
# Check Assumptions


```r
check_model(model2)
```

&lt;img src="regression_files/figure-html/unnamed-chunk-47-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Example: Assumption Alternatives

- If your assumptions go wrong:
    - Linearity - try nonlinear regression or nonparametric regression
    - Normality - more subjects, still fairly robust
    
    - Homogeneity/Homoscedasticity - bootstrapping
    
    - Multicolineary - drop one of the predictors; ridge regression
---

# Regression Model

- Is my overall model *(i.e., the regression equation)* useful at predicting the outcome variable?

    - Use the model summary, F-test, and `\(R^2\)`

- How useful are each of the individual predictors for my model?

    - Use the coefficients, t-test, and `\(pr^2\)`
---
# Overall Model

 Our overall model uses an *F*-test
 
- However, we can think about the hypotheses for the overall test being:

    - H0: We cannot predict the dependent variable
    
    - H1: We can predict the dependent variable
    
- Generally, this form does not include two tailed tests because the math is squared, so it is impossible to get negative values in the statistical test

---
# F-Statistic, Explained Over Unexplained

- F-statistics use measures of variance, which are sums of squares divided by relevant degrees of freedom

`$$F = \frac{SS_{Explained}/df1}{SS_{Unexplained}/df2} = \frac{MS_{Explained}}{MS_{Unexplained}}$$`

- If explained = unexplained, then F=1

- If explained &gt; then, F &gt;1 

- If explained &lt; unexplained, F &lt; 1

---
# ANOVA


```r
anova(lm(pitch~age, data=pitch))
```

```
## Analysis of Variance Table
## 
## Response: pitch
##           Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## age        1 5491.8  5491.8  71.166 5.504e-12 ***
## Residuals 64 4938.8    77.2                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
---

# Individual Predictors

- We test the individual predictors with a t-test:
  
    - `\(t = \frac{b}{SE}\)`
    - Therefore, the model for each individual predictor is our coefficient b. 
    - Single sample t-test to determine if the b value is different from zero
    
---
# Individual Predictors: Standardization 

- b = unstandardized regression coefficient

    - For every one unit increase in X, there will be b units increase in Y.

- `\(\beta\)` = standardized regression coefficient

    - b in standard deviation units
    - For every one SD increase in X, there will be `\(\beta\)` SDs increase in Y
    
- b or `\(\beta\)`?:

    - b is more interpretable given your specific problem
    
    - `\(\beta\)` is more interpretable given differences in scales for different variables
---

## Example: Overall Model

- Is the overall model significant? Yes!


```r
summary(model2)
```

```
## 
## Call:
## lm(formula = CESD_total ~ PIL_total + AUDIT_TOTAL_NEW + DAST_TOTAL_NEW, 
##     data = noout)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -19.857  -5.230  -1.231   3.540  28.724 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     56.25004    3.92276  14.339   &lt;2e-16 ***
## PIL_total       -0.38990    0.03406 -11.446   &lt;2e-16 ***
## AUDIT_TOTAL_NEW -0.09947    0.09437  -1.054   0.2928    
## DAST_TOTAL_NEW   1.02970    0.40030   2.572   0.0107 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 7.578 on 258 degrees of freedom
## Multiple R-squared:  0.3676,	Adjusted R-squared:  0.3602 
## F-statistic: 49.98 on 3 and 258 DF,  p-value: &lt; 2.2e-16
```

```r
library(papaja)
apa_style &lt;- apa_print(model2)
apa_style$full_result$modelfit
```

```
## $r2
## [1] "$R^2 = .37$, 90\\% CI $[0.28, 0.44]$, $F(3, 258) = 49.98$, $p &lt; .001$"
```

- `\(R^2 = .37\)`, 90\% CI `\([0.28, 0.44]\)`, `\(F(3, 258) = 49.98\)`, `\(p &lt; .001\)`

## Example: Predictors


```r
summary(model2)
```

```
## 
## Call:
## lm(formula = CESD_total ~ PIL_total + AUDIT_TOTAL_NEW + DAST_TOTAL_NEW, 
##     data = noout)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -19.857  -5.230  -1.231   3.540  28.724 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     56.25004    3.92276  14.339   &lt;2e-16 ***
## PIL_total       -0.38990    0.03406 -11.446   &lt;2e-16 ***
## AUDIT_TOTAL_NEW -0.09947    0.09437  -1.054   0.2928    
## DAST_TOTAL_NEW   1.02970    0.40030   2.572   0.0107 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 7.578 on 258 degrees of freedom
## Multiple R-squared:  0.3676,	Adjusted R-squared:  0.3602 
## F-statistic: 49.98 on 3 and 258 DF,  p-value: &lt; 2.2e-16
```

## Example: Predictors


```r
apa_style$full_result$PIL_total
```

```
## [1] "$b = -0.39$, 95\\% CI $[-0.46, -0.32]$, $t(258) = -11.45$, $p &lt; .001$"
```

```r
apa_style$full_result$AUDIT_TOTAL_NEW
```

```
## [1] "$b = -0.10$, 95\\% CI $[-0.29, 0.09]$, $t(258) = -1.05$, $p = .293$"
```

```r
apa_style$full_result$DAST_TOTAL_NEW
```

```
## [1] "$b = 1.03$, 95\\% CI $[0.24, 1.82]$, $t(258) = 2.57$, $p = .011$"
```

- Meaning: `\(b = -0.39\)`, 95\% CI `\([-0.46, -0.32]\)`, `\(t(258) = -11.45\)`, `\(p &lt; .001\)`
- Alcohol: `\(b = -0.10\)`, 95\% CI `\([-0.29, 0.09]\)`, `\(t(258) = -1.05\)`, `\(p = .293\)`
- Drugs: `\(b = 1.03\)`, 95\% CI `\([0.24, 1.82]\)`, `\(t(258) = 2.57\)`, `\(p = .011\)`

## Example: Predictors

- Two concerns:

    - What if I wanted to use beta because these are very different scales?
    - What about an effect size for each individual predictor?
---
# Beta

- You can use the `QuantPsyc` package for `\(\beta\)` values. 


```r
library(QuantPsyc)
lm.beta(model2)
```

```
##       PIL_total AUDIT_TOTAL_NEW  DAST_TOTAL_NEW 
##     -0.57578046     -0.06128615      0.15028366
```


```r
#easystats
standardize_parameters(model2)
```

```
## # Standardization method: refit
## 
## Parameter       | Std. Coef. |         95% CI
## ---------------------------------------------
## (Intercept)     |  -9.98e-17 | [-0.10,  0.10]
## PIL_total       |      -0.58 | [-0.67, -0.48]
## AUDIT_TOTAL_NEW |      -0.06 | [-0.18,  0.05]
## DAST_TOTAL_NEW  |       0.15 | [ 0.04,  0.27]
```
---
# Effect Size 

- R is the multiple correlation
- All overlap in Y, used for overall model
- `\(A+B+C/(A+B+C+D)\)`

&lt;img src="https://raw.githubusercontent.com/doomlab/learnSTATS/master/vignettes/pictures/regression/19.PNG" width="50%" style="display: block; margin: auto;" /&gt;

## Example: Effect Size 

- sr is the semi-partial correlations 
- Unique contribution of IV to `\(R^2\)` for those IVs

- Increase in proportion of explained Y variance when X is added to the equation

- `\(A/(A+B+C+D)\)`

&lt;img src="https://raw.githubusercontent.com/doomlab/learnSTATS/master/vignettes/pictures/regression/19.PNG" width="50%" style="display: block; margin: auto;" /&gt;
---
## Example: Effect Size 

- pr is the partial correlation

- Proportion in variance in Y not explained by other predictors but this X only

- `\(A/(A+D)\)`
- Pr &gt; sr

&lt;iframe src="https://github.com/doomlab/learnSTATS/blob/master/vignettes/pictures/regression/19.PNG" width="50%" height="400px" data-external="1"&gt;&lt;/iframe&gt;
--
# Partial Correlations

- We would add these to our other reports:

    - Meaning: `\(b = -0.39\)`, 95\% CI `\([-0.46, -0.32]\)`, `\(t(258) = -11.45\)`, `\(p &lt; .001\)`, `\(pr^2 = .30\)`
    - Alcohol: `\(b = -0.10\)`, 95\% CI `\([-0.29, 0.09]\)`, `\(t(258) = -1.05\)`, `\(p = .293\)`, `\(pr^2 &lt; .01\)`
    - Drugs: `\(b = 1.03\)`, 95\% CI `\([0.24, 1.82]\)`, `\(t(258) = 2.57\)`, `\(p = .011\)`, `\(pr^2 &lt; .01\)`


```r
library(ppcor)
partials &lt;- pcor(noout)
partials$estimate^2
```

```
##                    PIL_total  CESD_total AUDIT_TOTAL_NEW DAST_TOTAL_NEW
## PIL_total       1.000000e+00 0.336787039     0.007300578   1.883381e-07
## CESD_total      3.367870e-01 1.000000000     0.004287781   2.500593e-02
## AUDIT_TOTAL_NEW 7.300578e-03 0.004287781     1.000000000   2.640798e-01
## DAST_TOTAL_NEW  1.883381e-07 0.025005926     0.264079779   1.000000e+00
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": true,
"background-image": "url(\"lover.png\")",
"background-size": "cover"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
