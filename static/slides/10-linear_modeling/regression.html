<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>PSY 503: Foundations of Statistical Methods in Psychological Science</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jason Geller, Ph.D. (he/him/his)" />
    <script src="regression_files/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# PSY 503: Foundations of Statistical Methods in Psychological Science
]
.subtitle[
## Linear Modeling
]
.author[
### Jason Geller, Ph.D. (he/him/his)
]
.institute[
### Princeton University
]
.date[
### Updated:2022-10-14
]

---








# Outline

- Regression
- Multiple Regression
- Correlation
  - Pearson’s r
  - Spearman’s ρ
---
# What is Regression?

- A way of predicting the value of one variable from other variables.

    - It is a hypothetical model of the relationship between two or more variables
    
    - The model used is a linear one, but doesn't always have to be! 
    
    - Therefore, we describe the relationship using the equation of a straight line

---
# Describing a Straight Line

`$$Y_i = b_0 + b_1X_i + \varepsilon_i$$`
.pull-left[
- What is `\(b_i\)`?  

    - Regression coefficient for the predictor
    - Gradient (slope) of the regression line
    - Tells us how much we would expect y to change given a one-unit change in  x
    - Direction/Strength of Relationship
    
- What is `\(b_0\)`?

    - Intercept (value of Y when X(s) = 0)
    - Point at which the regression line crosses the Y-axis
]

.pull-right[

&lt;img src="images/riserun.png" width="75%" height="15%" style="display: block; margin: auto;" /&gt;

]
---
#The Best Fit Line and Least Squares

- The best fitting line is one that minimizes the squared difference between X and Y

- Ordinary least squares is most common

  - What do squares have to do with it? 
---
# Example

.pull-left[

```
## (Intercept) 
##    30.09886
```

```
##          hp 
## -0.06822828
```

&lt;img src="regression_files/figure-html/unnamed-chunk-3-1.gif" width="100%" style="display: block; margin: auto;" /&gt;
]

.pull-left[

- Shows two concepts:
  1. Regression line is "best fit line"
  2. The “best fit line” is the one that minimizes the sum of the squared deviations between each point and the line
]
---
# Visualizing Error

&lt;img src="regression_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
# Visualize Errors as Sqaures

.pull-left[


```r
some_data &lt;- data.frame(Y= c(1,2,4,3,5,4,6,5),
                        X= c(3,5,4,2,6,7,8,9)) %&gt;%
  mutate(Y_pred = predict.lm(lm(Y~X))) %&gt;%
  mutate(Y_error = Y - Y_pred)

g=ggplot(some_data, aes(x=X, y=Y))+
  geom_point()+
  geom_smooth(method='lm', se=FALSE)+
  geom_point(aes(y=Y_pred), color='red') +
  geom_segment(aes(xend = X, yend = Y-Y_error), alpha = .5)+
  geom_rect(aes(ymin=Y, 
                ymax=Y_pred, 
                xmin=X,
                xmax=X+Y_error), 
            alpha = .2)
```

]

.pull-right[

&lt;img src="regression_files/figure-html/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]
---
# Worse Fit Lines

&lt;img src="regression_files/figure-html/unnamed-chunk-7-1.png" width="100%" /&gt;
---
# Simple Regression Example

- Pitch perception and age 

  - As one ages, ability to hear certain pitch frequencies decreases

&lt;img src="regression_files/figure-html/unnamed-chunk-8-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---


```r
lm(pitch~age, data=pitch)
```

```
## 
## Call:
## lm(formula = pitch ~ age, data = pitch)
## 
## Coefficients:
## (Intercept)          age  
##    215.9581      -0.6936
```

&lt;img src="regression_files/figure-html/unnamed-chunk-10-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
&lt;img src="regression_files/figure-html/unnamed-chunk-11-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
&lt;img src="regression_files/figure-html/unnamed-chunk-12-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
$$ pitch = 216 + (-.7)*age$$

&lt;img src="regression_files/figure-html/unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
$$ \hat{pitch} = 216 + (-.7)*40$$
&lt;img src="regression_files/figure-html/unnamed-chunk-14-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

$$ pitch = 216 - 28$$

&lt;img src="regression_files/figure-html/unnamed-chunk-15-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
# Partitoning Variance


`$$residual = y - \hat{y} = y - (x*\hat{\beta_x} + \hat{\beta_0})$$`

`$$SS_{error} = \sum_{i=1}^n{(y_i - \hat{y_i})^2} = \sum_{i=1}^n{residuals^2}$$`
`$$SS_{total}=\sum_{i=1}^{n}(Y_i-\bar{Y_{i}})^2$$`

`$$MS_{error} = \frac{SS_{error}}{df} = \frac{\sum_{i=1}^n{(y_i - \hat{y_i})^2} }{N - p}$$`
`$$SE_{model} = \sqrt{MS_{error}}SE_{model} = \sqrt{MS_{error}}$$`

`$$SE_{\hat{\beta}_x} = \frac{SE_{model}}{\sqrt{{\sum{(x_i - \bar{x})^2}}}}$$`
---

# Regression Model

- Is my overall model *(i.e., the regression equation)* useful at predicting the outcome variable?

    - Use the model summary, F-test, and `\(R^2\)`

- How useful are each of the individual predictors for my model?

    - Use the coefficients, t-test, and `\(pr^2\)`
    
---
# Overall Model

 Our overall model uses an *F*-test
 
- However, we can think about the hypotheses for the overall test being:

    - H0: We cannot predict the dependent variable. 
    - H1: We can predict the dependent variable. 

- Generally, this form does not include one tailed tests because the math is squared, so it is impossible to get negative values in the statistical test
---
# F-Statistic, Explained Over Unexplained

- F-statistics use measures of variance, which are sums of squares divided by relevant degrees of freedom

`$$F = \frac{SS_{Explained}/df1}{SS_{Unexplained}/df2} = \frac{MS_{Explained}}{MS_{Unexplained}}$$`

- If explained = unexplained, then F=1

- If explained &gt; then, F &gt;1 

- If explained &lt; unexplained, F &lt; 1

---
# Individual Predictors

- We test the individual predictors with a t-test:
  
    - `\(t = \frac{b}{SE}\)`
    - Therefore, the model for each individual predictor is our coefficient b. 
    - Single sample t-test to determine if the b value is different from zero
    
---
# Individual Predictors: Standardization 

- b = unstandardized regression coefficient

    - For every one unit increase in X, there will be b units increase in Y.

- `\(\beta\)` = standardized regression coefficient

    - b in standard deviation units.
    - For every one SD increase in X, there will be `\(\beta\)` SDs increase in Y.
    
- b or `\(\beta\)`?:

    - b is more interpretable given your specific problem
    
    - `\(\beta\)` is more interpretable given differences in scales for different variables.
---
# Individual Predictors: Understand the NHST

- Therefore, we might use the following hypotheses:

    - H0: age does not predict pitch loss.
    - H1: age  does predict pitch loss. 

- Or, we could use a directional test, since the test statistic *t* can be negative:

    - H0: Age  negatively or does not predict Y (b &lt;= 0).
    - H1: Age  positively predicts Y (b &gt; 0).

---
# Individual Predictors

 Unlike correlation, these statistics are often reported with *t*(df). 
- `\(df = N - k - 1\)`

    - N = total sample size
    - k = number of predictors
    - Correlation is technically N - 1 - 1 = N - 2
    - We can also find this value in our output by looking at the *F*-statistic.
---
# `broom` Regression 

tidy(): coefficent table
glance(): model summary
augment(): adds information about each observation


```r
library(broom)
library(report)

fit.pitch = lm(pitch~age, data=pitch)
```
---


```r
tidy(fit.pitch)
```

```
## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)  216.       4.95       43.6  2.49e-49
## 2 age           -0.694    0.0822     -8.44 5.50e-12
```

---

```r
tidy(fit.pitch) %&gt;%
  select(term, estimate)
```

```
## # A tibble: 2 × 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)  216.   
## 2 age           -0.694
```

---
# Reporting

- We fitted a linear model (estimated using OLS) to predict pitch with age (formula: pitch ~ age). The model explains a statistically significant and substantial proportion of variance (R2 = 0.53, F(1, 64) = 71.17, p &lt; .001, adj. R2 = 0.52). The model's intercept, corresponding to age = 0, is at 215.96 (95% CI [206.07, 225.85], t(64) = 43.63, p &lt; .001). Within this model:

  - The effect of age is statistically significant and negative (beta = -0.69, 95% CI [-0.86, -0.53], t(64) = -8.44, p &lt; .001; Std. beta = -0.73, 95% CI [-0.90, -0.55])

Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using a Wald t-distribution approximation.

---
# Example: Overall Model

- Is the overall model significant? Yes!


```r
library(papaja)
apa_style &lt;- apa_print(fit.pitch)
apa_style$full_result$modelfit
```

```
## $r2
## [1] "$R^2 = .53$, 90\\% CI $[0.37, 0.65]$, $F(1, 64) = 71.17$, $p &lt; .001$"
```

- `\(R^2 = .53\)`, 90\% CI `\([0.37, 0.65]\)`, `\(F(1, 64) = 71.17\)`, `\(p &lt; .001\)`
---
#  Example: Predictors


```r
apa_style$full_result$age
```

```
## [1] "$b = -0.69$, 95\\% CI $[-0.86, -0.53]$, $t(64) = -8.44$, $p &lt; .001$"
```
---

# Residuals, Fitted Values, and Model Fit

- If we want to make inferences about the regression parameter estimates, then we also need an estimate of their variability. 

---
# SS Unexplained (Sums of Sqaures Error)

`$$residual = y - \hat{y} = y - (x*\hat{\beta_x} + \hat{\beta_0})$$`

`$$SS_{error} = \sum_{i=1}^n{(y_i - \hat{y_i})^2} = \sum_{i=1}^n{residuals^2}$$`

&lt;img src="regression_files/figure-html/unnamed-chunk-21-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# SST 

&gt; squared differences between the observed dependent variable and its mean. 

`$$SS_{total} = \sum{(Y_i - \bar{Y})^2}$$`

&lt;img src="regression_files/figure-html/unnamed-chunk-22-1.png" width="100%" /&gt;


---
# Sums of Squares Regression (SS Explained)

&gt; The sum of the differences between the predicted value and the mean of the dependent variable

`$$SS_{Explained} = \sum (Y'_i - \bar{Y})^2$$`

&lt;img src="regression_files/figure-html/unnamed-chunk-23-1.png" width="100%" style="display: block; margin: auto;" /&gt;
---
# All Together


```r
library(patchwork)

(total_plot +plot_spacer())/(exp_plot+res_plot)+
  plot_annotation(title = 'SStotal = SSexplained + SSunexplained')
```

&lt;img src="regression_files/figure-html/unnamed-chunk-24-1.png" width="36%" /&gt;
---
# Standard Error

`$$MS_{error} = \frac{SS_{error}}{df} = \frac{\sum_{i=1}^n{(y_i - \hat{y_i})^2} }{N - p}$$`
`$$SE_{model} = \sqrt{MS_{error}}SE_{model} = \sqrt{MS_{error}}$$`

`$$SE_{\hat{\beta}_x} = \frac{SE_{model}}{\sqrt{{\sum{(x_i - \bar{x})^2}}}}$$`
---

```r
# get MSE
mse=performance_mse(fit.pitch)

SEM&lt;-sqrt(mse)

ssr &lt;- sum((pitch$age - mean(pitch$age))^2)

SE &lt;- mse/sqrt(ssr)
```
---
# Effect Size: `\(R^2\)`

`$$R^2 = 1 - \frac{SS_{\text{error}}}{SS_{\text{tot}}}$$`
`$$R^2 = 1 - \frac{SS_{unexplained}}{SS_{Total}} = \frac{SS_{explained}}{SS_{Total}}$$`

- Standardized effect size

  - Amount of variance explained
  
  - Range: 0-1
  
- Take *r* and `\(r^2\)`
---
# Statistical Tests

`$$\begin{array}{c}
t_{N - p} = \frac{\hat{\beta} - \beta_{expected}}{SE_{\hat{\beta}}}\\
t_{N - p} = \frac{\hat{\beta} - 0}{SE_{\hat{\beta}}}\\
t_{N - p} = \frac{\hat{\beta} }{SE_{\hat{\beta}}}
\end{array}$$`

---

```r
# Change the point sizes manually
ggplot(pitch, aes(x=age, y=pitch))+
   geom_point()+ 
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  theme(legend.position="top")
```

&lt;img src="regression_files/figure-html/unnamed-chunk-26-1.png" width="36%" /&gt;

---

```r
d=augment(fit.pitch)# residuals and fitted values
```
---

---
# Regression Assumptions

- Outliers
- Linearity
- Normality of residuals
- Homogeneity of residual variance (“homoskedasticity”)
- Independence of residuals

---
# Assumptions

- Normality

- Applies to residuals and not the distribution of the data
 

```r
 ggplot(pitch, aes(age)) +
  geom_density(color = 4,
               fill = 4,
               alpha = 0.25)
```

&lt;img src="regression_files/figure-html/unnamed-chunk-28-1.png" width="36%" /&gt;
---
# Example: Outliers 

- In this section, we will add a few new outlier checks:

    - Mahalanobis
    - Leverage scores
    - Cook's distance

- Because we are using regression as our model, we may consider using multiple checks before excluding outliers. 
---
# Example: Mahalanobis 

- The `mahalanobis()` function we have used previously.
- Since we are going to use multiple criteria, we are going to save if they are an outlier or not

- The table tells us: 0 (not outliers) and 1 (considered an outlier) for just Mahalanobis values


```r
mahal &lt;- mahalanobis(nomiss, 
                    colMeans(nomiss), 
                    cov(nomiss))
cutmahal &lt;- qchisq(1-.001, ncol(nomiss))
badmahal &lt;- as.numeric(mahal &gt; cutmahal) ##note the direction of the &gt; 
table(badmahal)
```
---
# Example: Other Outliers

- To get the other outlier statistics, we have to use the regression model we wish to test. - We will use the `lm()` function with our regression formula.
- `Y ~ X + X`: Y is approximated by X plus X.
- So we will predict depression scores (CESD) with meaning, drugs, and alcohol.

---
# Example: Leverage 

- **Definition** - influence of that data point on the slope
- Each score is the change in slope if you exclude that data point
- How do we calculate how much change is bad?

    - `\(\frac{2K+2}{N}\)`
    - K is the number of predictors
    - N is the sample size 
---
#  Example: Leverage 


```r
k &lt;- 3 ##number of IVs
leverage &lt;- hatvalues(fit.pitch)
cutleverage &lt;- (2*k+2) / nrow(pitch)
badleverage &lt;- as.numeric(leverage &gt; cutleverage)
table(badleverage)
```

```
## badleverage
##  0  1 
## 63  3
```
---
# Example: Cook's Distance 

- Influence **(Cook's Distance)** - a measure of how much of an effect that single case has on the whole model 
- Often described as leverage + discrepancy 
- How do we calculate how much change is bad?
  
    - `\(\frac{4}{N-K-1}\)`


```r
cooks &lt;- cooks.distance(fit.pitch)
cutcooks &lt;- 4 / (nrow(pitch) - k - 1)
badcooks &lt;- as.numeric(cooks &gt; cutcooks)
table(badcooks)
```

```
## badcooks
##  0  1 
## 60  6
```
---
# Example: Outliers Combined

- What do I do with all these numbers? 
- Create a total score for the number of indicators a data point has.
- You can decide what rule to use, but a suggestion is 2 or more indicators is an outlier.


```r
##add them up!
totalout &lt;- badmahal + badleverage + badcooks
table(totalout)
noout &lt;- subset(pitch, totalout &lt; 2)
```
---
# Example: Assumptions

- Now that we got rid of outliers, we need to run that model again, without the outliers.


```r
fit.pitch &lt;- lm(pitch~age, data=pitch)
```
---
# Check Assumptions


```r
check_normality(fit.pitch)
```

```
## OK: residuals appear as normally distributed (p = 0.483).
```

```r
check_heteroscedasticity(fit.pitch)
```

```
## OK: Error variance appears to be homoscedastic (p = 0.078).
```

```r
check_autocorrelation(fit.pitch)
```

```
## OK: Residuals appear to be independent and not autocorrelated (p = 0.080).
```

```r
check_collinearity(fit.pitch)
```

```
## NULL
```

```r
# check_model(fit.pitch)
```
---
# Example: Assumption Alternatives

- If your assumptions go wrong:

    - Linearity - try nonlinear regression or nonparametric regression
    - Normality - more subjects, still fairly robust
    - Homogeneity/Homoscedasticity - bootstrapping
---


```r
d=augment(fit.pitch)# residuals and fitted values
```

---

```r
fit.pitch &lt;- lm(pitch~1 + age, data=pitch)

fit.pitch
```

```
## 
## Call:
## lm(formula = pitch ~ 1 + age, data = pitch)
## 
## Coefficients:
## (Intercept)          age  
##    215.9581      -0.6936
```
---
# `broom` Regression 

tidy(): coefficent table
glance(): model summary
augment(): adds information about each observation


```r
library(broom)
library(report)
```
---


```r
tidy(fit.pitch)
```

```
## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)  216.       4.95       43.6  2.49e-49
## 2 age           -0.694    0.0822     -8.44 5.50e-12
```

---

```r
tidy(fit.pitch) %&gt;%
  select(term, estimate)
```

```
## # A tibble: 2 × 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)  216.   
## 2 age           -0.694
```

# `\(R^2\)`

```r
library(performance)

r2(fit.pitch)
```

```
## # R2 for Linear Regression
##        R2: 0.527
##   adj. R2: 0.519
```

---
# Reporting

- We fitted a linear model (estimated using OLS) to predict pitch with age (formula: pitch ~ 1 + age). The model explains a statistically significant and substantial proportion of variance (R2 = 0.53, F(1, 64) = 71.17, p &lt; .001, adj. R2 = 0.52). The model's intercept, corresponding to age = 0, is at 215.96 (95% CI [206.07, 225.85], t(64) = 43.63, p &lt; .001). Within this model:

  - The effect of age is statistically significant and negative (beta = -0.69, 95% CI [-0.86, -0.53], t(64) = -8.44, p &lt; .001; Std. beta = -0.73, 95% CI [-0.90, -0.55])

Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using a Wald t-distribution approximation.
---
class: middle
# Multiple Predictors
---




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": true,
"background-image": "url(\"lover.png\")",
"background-size": "cover"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
