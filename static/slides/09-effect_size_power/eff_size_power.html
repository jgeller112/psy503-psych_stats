<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>PSY 503: Foundations of Statistics in Psychological Science</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jason Geller, Ph.D. (he/him/his)" />
    <script src="eff_size_power_files/header-attrs/header-attrs.js"></script>
    <link href="eff_size_power_files/panelset/panelset.css" rel="stylesheet" />
    <script src="eff_size_power_files/panelset/panelset.js"></script>
    <link href="eff_size_power_files/countdown/countdown.css" rel="stylesheet" />
    <script src="eff_size_power_files/countdown/countdown.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# PSY 503: Foundations of Statistics in Psychological Science
]
.subtitle[
## Effect Size and Power
]
.author[
### Jason Geller, Ph.D. (he/him/his)
]
.institute[
### Princeton University
]
.date[
### Last Updated: 2022-10-11
]

---





<style>.panelset{--panel-tab-foreground: honeydew;--panel-tab-background: seagreen;}</style>
&lt;style type="text/css"&gt;
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 100px;
}
&lt;/style&gt;

&lt;style type="text/css"&gt;
.scroll-100 {
  max-height: 100px;
  overflow-y: auto;
  background-color: inherit;
}
&lt;/style&gt;
# Knowledge Check

<div class="countdown" id="timer_768f710c" data-update-every="1" tabindex="0" style="right:0;bottom:0;">
<div class="countdown-controls"><button class="countdown-bump-down">&minus;</button><button class="countdown-bump-up">&plus;</button></div>
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>


```r
library(report)

df=tibble::tribble(
  ~Fake.Name, ~Own.Name,
         60L,       47L,
         78L,       63L,
         57L,       27L,
         64L,       42L,
         89L,       23L,
         79L,       75L,
         63L,       24L,
         81L,       44L,
         46L,       40L,
         50L,       44L
  )
```
Males completed a math test under their own name and a fake name.

- What test should be run?
- Check assumptions
- Run the test. What can we conclude?
---

# Knowledge Check Answer 


```r
shapiro.test(df$Own.Name) # check norm 
```

```
## 
## 	Shapiro-Wilk normality test
## 
## data:  df$Own.Name
## W = 0.91469, p-value = 0.3148
```

```r
shapiro.test(df$Fake.Name) # check norm
```

```
## 
## 	Shapiro-Wilk normality test
## 
## data:  df$Fake.Name
## W = 0.94695, p-value = 0.6326
```

```r
t_test=t.test(df$Fake.Name, df$Own.Name, paired = TRUE) # paired
```

---
# Knowledge Check Answer

```r
t_test=t.test(df$Fake.Name, df$Own.Name, paired = TRUE) # paired
```

- Effect sizes were labelled following Cohen's (1988) recommendations.

The Paired t-test testing the difference between df$Fake.Name and df$Own.Name (mean difference = 23.80) suggests that the effect is positive, statistically significant, and large (difference = 23.80, 95% CI [9.80, 37.80], t(9) = 3.85, p = 0.004; Cohen's d = 1.22, 95% CI [0.37, 2.03])
---
# Last Class

- Two sample *t*-tests

  - Independent
  
  - Dependent (paired)
  
- Non-parametric tests

- Multiple comparisons
---
# Today

- Effect size

- Statistical Power

  - What is Power?

  - Why do we care about power?

  - Determining Power
  
      - R packages 

---
class:middle center
# Effect Size

- "The amount of anything that's of research interest" (Cumming &amp; Calin-Jageman)
---
# Effect Size

- Simple way to quantify the difference between two means / groups, by emphasizing the size of the difference rather than confounding with the sample size (like p-values)

- Small

&lt;img src="rpsychologist-cohend.svg" width="80%" style="display: block; margin: auto;" /&gt;

---
# Effect Size

- Simple way to quantify the difference between two means / groups, by emphasizing the size of the difference rather than confounding with the sample size (like p-values)

- Medium

&lt;img src="rpsychologist-cohend (1).svg" width="80%" style="display: block; margin: auto;" /&gt;

---

# Effect Size

- Simple way to quantify the difference between two means / groups, by emphasizing the size of the difference rather than confounding with the sample size (like p-values)

- Large
&lt;img src="rpsychologist-cohend (2).svg" width="80%" style="display: block; margin: auto;" /&gt;
---
# Effect Size

- Effect size options:

    - Cohen's *d* 
    - Hedges' *g* (*N* &lt; 20)
    - Glass' `\(\Delta\)`
    - *r*
  
- Always report an effect size with statistics! 
---
# Cohen's *d*: 1 Sample

`$$d = \frac{\bar{X}-\mu}{s}$$`

- d = effect size
- `\(\bar{X}\)`= sample mean
- `\(\mu_0\)`= population mean 
- s = sample standard deviation

---
# Open R  

- From lecture: A cookie company claims that there are 15 chocolate chips per cookie, but you aren’t convinced. You take 10 cookies and count the number of chocolate chips in each cookie. Here is  the data: [13,14,15,17,18,19,21,20,19,20]. Are the number of chocolate chips significantly different from 15 (reminder: yes, p = 0.015)? Report the effect size.

<div class="countdown" id="timer_fb83ee16" data-update-every="1" tabindex="0" style="right:0;bottom:0;">
<div class="countdown-controls"><button class="countdown-bump-down">&minus;</button><button class="countdown-bump-up">&plus;</button></div>
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">15</span></code>
</div>

---
# Open R  


```r
data1 = c(13,14,15,17,18,19,21,20,19,20)
X = mean(data1) 
s = sd(data1)
mu = 15
d = (X - mu) / s
abs(d) # report positive value
```

```
## [1] 0.9431191
```
---
# Effect Size Packages


```r
library(effectsize) # easystats package
library(report) # report results
library(MOTE) # erin package effect size

cookie=t.test(data1, mu=15) # perform ttest

effectsize::cohens_d(cookie, data = data1, mu=15) # get cohens_d 
```

```
## Cohen's d |       95% CI
## ------------------------
## 0.94      | [0.17, 1.68]
## 
## - Deviation from a difference of 15.
```

```r
effectsize::hedges_g(cookie, data = data1, mu=15)
```

```
## Hedges' g |       95% CI
## ------------------------
## 0.86      | [0.16, 1.54]
## 
## - Deviation from a difference of 15.
```

```r
# use when N &lt; 20 bias corrected
```
---
- Effect sizes were labelled following Cohen's (1988) recommendations.

The One Sample t-test testing the difference between data1 (mean = 17.60) and mu = 15 suggests that the effect is positive, statistically significant, and large (difference = 2.60, 95% CI [15.63, 19.57], t(9) = 2.98, p = 0.015; Cohen's d = 0.94, 95% CI [0.17, 1.68])
---
# Cohen's *d*: 2 Sample

`$$d = \frac{\bar{X_1} - \bar{X_2}}{SD_{pooled}}$$`

- d = effect size
- `\(\bar{X}\)`= sample mean
- `\(\mu_0\)`= population mean 
- s = sample standard deviation

---
Cohen’s *d*: 2 sample

From last lecture: A math test was given to 300 17 year old students in 1978 and again to another 350 17 year old students in 1992

Group 1: X1 = 300.4, s1 = 34.9, n = 300
Group 2: X2 = 306.7, s2 = 30.1, n = 350

- What is the effect size?

<div class="countdown" id="timer_11d6acb3" data-update-every="1" tabindex="0" style="right:0;bottom:0;">
<div class="countdown-controls"><button class="countdown-bump-down">&minus;</button><button class="countdown-bump-up">&plus;</button></div>
<code class="countdown-time"><span class="countdown-digits minutes">00</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">15</span></code>
</div>

---
# Open R  


```r
group1 &lt;- rnorm(300, mean = 300.4, sd=34.9) 

group2 &lt;- rnorm(350, mean = 306.7, sd=30.1)

r=t.test(group1, group2)

effectsize::cohens_d(r, data = data1, pooled_sd = TRUE)#nonwelch
```

```
## Cohen's d |         95% CI
## --------------------------
## -0.27     | [-0.42, -0.11]
## 
## - Estimated using un-pooled SD.
```

```r
effectsize::cohens_d(r, data = data1, pooled_sd = FALSE)#welch
```

```
## Cohen's d |         95% CI
## --------------------------
## -0.27     | [-0.42, -0.11]
## 
## - Estimated using un-pooled SD.
```
---
# Reporting 

- Effect sizes were labelled following Cohen's (1988) recommendations.

The Welch Two Sample t-test testing the difference between group1 and group2 (mean of x = 298.26, mean of y = 306.85) suggests that the effect is negative, statistically significant, and small (difference = -8.58, 95% CI [-13.60, -3.57], t(588.23) = -3.36, p &lt; .001; Cohen's d = -0.27, 95% CI [-0.42, -0.11])
---
# Cloak vs. No Cloak Data: Effect Size

- What is the effect size? 


```r
longdata &lt;- read_csv("https://raw.githubusercontent.com/doomlab/statsofdoom-files/master/graduate/R%20Flip/11_ttests/data/invisible.csv")
```

---
# Cloak vs. No Cloak Data: Effect Size


```r
library(MOTE)# erin effect size package 
library(rio)

# extract means and sd from cloak df
 M &lt;- longdata  %&gt;%
   group_by(Cloak) %&gt;%
   dplyr::summarize(mean=mean(Mischief), sd=sd(Mischief), N =12)

# run indep t test using MOTE             
effect &lt;- d.ind.t(m1 = M$mean[1], m2 = M$mean[2],        
                  sd1 = M$sd[1], sd2 = M$sd[2],        
                  n1 = 12, n2 = 12, a = .05)
effect$d # get effect size
```

```
## [1] 0.6995169
```


```r
# using effect size package
library(effectsize) # effect size package 
cohens_d(Mischief~Cloak, data = longdata)
```

```
## Cohen's d |        95% CI
## -------------------------
## 0.70      | [-0.13, 1.52]
## 
## - Estimated using pooled SD.
```
---
# Cloak vs. No Cloak: Effect Size

- While our statistical test indicated no differences, the effect size indicates a medium difference between means

- This difference in interpretation is likely due to low power with a small sample size 
---
# Cohen's *d*: Dependent

- Lots of different ones

  - State which one you are using! 

- `\(D_{avg}\)` - looks at both SDs without controlling for r

`$$d_{avg} = \frac{\overline{M_1} - \overline{M_2}}
{\sqrt{\frac{(\sigma_1^2 + \sigma_2^2)}{2}}}$$`

-  `\(d_z\)` - we would overestimate the effect size
`$$d_{z} = \frac{t}
{\sqrt{n}}$$`

---
# Dependent: Effect Size (Lakens, 2013)

- `\(d_{rm}\)` - looks at both SDs and controls for r

`$$d_{rm} = \frac{M_1 - M_2}{\sqrt{(SD_1^2 + SD_2^2)  - (2 \times r \times SD_1 \times SD_2)}} \times \sqrt{2 \times (1-r)}$$`
- You do not normally have to calculate all of these, just showing how these are different
---
# R 

<div class="countdown" id="timer_df48b2d6" data-update-every="1" tabindex="0" style="right:0;bottom:0;">
<div class="countdown-controls"><button class="countdown-bump-down">&minus;</button><button class="countdown-bump-up">&plus;</button></div>
<code class="countdown-time"><span class="countdown-digits minutes">05</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">15</span></code>
</div>

Using the cloak data, calculate *d* for dependent samples

Use: 

- `MOTE` d.dep.t.avg(m1, m2, sd1, sd2, n, a = 0.05) ($d_{avg}$)

- `MOTE` d.dep.t.rm(m1, m2, sd1, sd2, r, n, a = 0.05) ($d_{rm}$)

- What Cohen's *d* measure is used in the `effectsize` package?
---
# `\(d_{avg}\)`




```r
effect2 &lt;- d.dep.t.avg(m1 = M$mean[1], m2 = M$mean[2],      
                  sd1 = M$sd[1], sd2 = M$sd[2],        
                  n=12, a = .05)
effect2$d
```

```
## [1] 0.7013959
```

---
# `\(d_{rm}\)`


```r
#MOTE
effect2 &lt;- d.dep.t.rm(m1 = M$mean[1], m2 = M$mean[2],        
                  sd1 = M$sd[1], sd2 = M$sd[2],        
                  n=12, a = .05,  r= .7)

effect2$d
```

```
## [1] 0.6909434
```
---
# `\(d_z\)`


```r
#MOTE

diff &lt;- longdata$Mischief[longdata$Cloak == "Cloak"] - longdata$Mischief[longdata$Cloak == "No Cloak"]

effect2.1 = d.dep.t.diff(mdiff = mean(diff, na.rm = T), 
                         sddiff = sd(diff, na.rm = T),
                         n = length(diff), a = .05)
effect2.1$d
```

```
## [1] 1.098244
```
---
# Effect Size Interpretation

- Cohen (1988)

  - d &lt; 0.2 - Very small

  - 0.2 &lt;= d &lt; 0.5 - Small

  - 0.5 &lt;= d &lt; 0.8 - Medium

  - d &gt;= 0.8 - Large 

---
class: middle

# Power
---
# Recap of NHST

- Do invisibility cloaks increase mischeivous behavior?

  - `\(H_0\)`:There is no effect of cloaks on behavior
  - `\(H_1\)`: There is an effect of cloaks on behavior
  
---
# Recap of NHST

- A world in which `\(H_1\)` exists

  - Two types of errors: 

&lt;img src="des.png" width="50%" style="display: block; margin: auto;" /&gt;
---
# Power

- Power `\(1-\beta\)` : Probability rejecting null when it is false 

- **Detecting the effect when it really exists**

&lt;img src="type1dis.jpg" width="50%" style="display: block; margin: auto;" /&gt;

---
# What is common?

In psychology: 

- β = .20

 - This means we are willing to make a Type II error 20% of the time (i.e., 80% power).

- α =.05
 
  - This means we are willing to make a Type I error only 5% of the time (i.e., significance &lt; .05.

- 1−β = .80 (should be .9)
---
# Power

What does it mean if we say: "we compare retrieval practice to re-reading
with power = .75"

--

 - **If retrieval practice is actually beneficial, there is a
75% chance we’ll get a significant result when we do this study MANY MANY TIMES**

--

- We compare bilinguals to monolinguals on a
test of non-verbal cognition with power = .35

--

  - **If there is a difference between monolinguals &amp; bilinguals, there is a 35% chance we’ll get p &lt; .05 IF WE DO THIS MANY MANY TIMES**
---
# Why do we care?

- Efficient use of resources

  - Power analyses tell us if our planned sample size (n) is:

    - Large enough to be able to find what    we’re looking for

     - Not too large that we’re collecting more data than necessary 

- This is about good use of our resources!

    - Societal resources: money, participant hours

    - Your resources: Time!!
  
---
# Why do we care?

- Avoid p-hacking (Simmons et al., 2011)

  - Rate of false positive results increases if we keep collecting data whenever our effect is non-sig
  
 - Power analysis decides sample in advance 
 
---
# Why do we care?

.pull-left[

- Understand non-replication (Open Science
Collaboration, 2015)

  - Even if an effect exists in the population, we’d expect some non-significant results
  
      - Power is almost never 100%
      
      - In fact, many common designs in psychology have low power (Etz &amp; Vandekerckhove, 2016; Maxwell et al., 2015)
] 

.pull-right[

&lt;img src="rep.jpg" width="100%" style="display: block; margin: auto;" /&gt;
]
---
# Why do we care?

- Understand null results

  - Non-significant result, by itself, doesn’t prove an effect doesn’t exist

  - With high power, null result is more informative

      -  E.g., null effect of cloaks on behavior 20% power
      
          -  Maybe cloaks work &amp; we just couldn’t detect the effect?

           - But: null effect of cloaks on behavior with  power of 90%
         
          - Makes me more sure 
  
---
# Why do we care?

- Granting agencies want them now 

  - Don’t want to fund a study with low probability of showing anything
  
      - e.g., Our theory predicts greater activity in Broca’s area in condition A than condition B. But our experiment has only a 16% probability of detecting the difference. Not good!
---
# Why do we care?

- Scientific accuracy!

  - If there is an effect, we want to know about it!
---
# Power Analysis

- Power analysis: Do we have the
power to detect the effect we’re interested in? If not, what is it going to take?

- Depends on: 

  1. Sample size
  2. Effect size (e.g., d)
  3. Statistical significance criteria (α)
  4. Variability

---
# Power to Estimate Sample Size

- Calculate required sample size given a) effect size (e.g., d) b) significance level (α), c) desired power.

- How do we determine the effect size?

1. Smallest effect size of interest (SESOI) (e.g., d = 0.5) (Lakens)

--

2. A priori

  -  Use literature to estimate effect size (set α, desired power)

--
3. Pilot data

  - Estimate effect size using pilot data (set α, desired power)
---
class: middle, center
**For SESOI, you consider the smallest effect size you care about. For both a priori and pilot data power analyses, you need to get an estimate of the effect size (e.g., d)**

---
# Determining Power

## R Packages

.pull-left[
- `pwr` package
]

.pull-right[
&lt;img src="pwr_package.jpg" width="50%" style="display: block; margin: auto;" /&gt;
]
- Enter three of the four parameter options above (sample size, effect size, statistical significance, and power) and the package will calculate the fourth parameter.
---
# Determining Power

## R Packages: Power
- `Superpower`
  - https://aaroncaldwell.us/SuperpowerBook/


```r
#install.packages("Superpower")
library(Superpower)
```

- `Mixedpower`(simulations for more complicated models)

```r
# if (!require("devtools")) {
 #   install.packages("devtools", dependencies = TRUE)}

 #  devtools::install_github("DejanDraschkow/mixedpower") 
   
library(mixedpower)
```

---
# A Priori Power

From last lecture: A math test was given to 17 year old students in 1978 and again to another 17 year old students in 1992. From that data we have an estimate of the following parameters

- Group 1: X1 = 300.4, s1 = 34.9, n = 300
- Group 2: X2 = 306.7, s2 = 30.1, n = 350

We want to conduct a similar experiment and estimate how many people we should collect to achieve a desired power of 80%

- What is the effect size?

- Calculate power

---


```r
library(pwr)
d2abs = .2
pwr.t.test()
```
---
# Power Curves

- Visualization showing power as a function sample size


```r
d2abs=.2

p.out &lt;- pwr.t.test(d = d2abs, power = 0.80, sig.level = 0.05,
type = "two.sample", alternative = "two.sided")

plot(p.out)
```

&lt;img src="eff_size_power_files/figure-html/unnamed-chunk-30-1.png" width="50%" style="display: block; margin: auto;" /&gt;
---

- From last lecture: A cookie company claims that there are 15 chocolate chips per cookie, but you aren’t convinced. You take 10 cookies and count the number of chocolate chips in each cookie. Here is what the data looks like: [13,14,15,17,18,19,21,20,19,20]. Are the number of chocolate chips significantly different from 15 (reminder: yes, p = 0.015)?

How many cookies do you need to get 90% power?

---

```r
data1 = c(13,14,15,17,18,19,21,20,19,20)

r=t.test(data1, mu=15)

d1=effectsize::cohens_d(r, data = data1, mu=15)
```

---

```r
pwr.t.test(d = d1$Cohens_d, power = 0.90, sig.level = 0.05, type = "one.sample", alternative = "two.sided")$n
```

```
## [1] 13.87667
```
---
# Power Questions

- What would our power be if I could only collect 3 cookies?

- Can calculate current power by slightly adjusting the function
---

```r
data1 = c(13,14,15,17,18,19,21,20,19,20)

pwr.t.test(n = , d = , power = , sig.level = 0.05,
type = "one.sample", alternative = "two.sided")$power
```
---
Power Question

- Using the cookie data, how many cookies would we need to achieve 90% power?

---


```r
library(pwr)
pwr.t.test(n =NULL, d =, #effect size
           sig.level = .05, #alpha
           power = , #power 
           type = "one.sample", #independent
           alternative = "two.sided") #two tailed test
```
---
# Simulation

Remember the definition of power?

- The probability of observing a significant effect in our sample if the effect truly exists in the population

- What if we knew for a fact that the effect existed in a particular population?

  - Then, a measure of power is how often we get a significant result in a sample (of our intended n)

- Solution: We create (“simulate”) the data.
---
# Monte Carlo Simulations

1. Set population parameters
 
 **from already conducted studies or pilot data**

---
# Cloak Data


```r
design &lt;- "2b"
n &lt;- 12
mu &lt;- c(5, 3.75)
sd &lt;- c(1.65, 1.91)
label_list = list("condition" = c("Cloak", "No Cloak")) #

design_result &lt;- ANOVA_design(design = design,
                              n = n,
                              mu = mu, 
                              sd = sd, 
                              label_list = label_list)
```

&lt;img src="eff_size_power_files/figure-html/unnamed-chunk-35-1.png" width="36%" style="display: block; margin: auto;" /&gt;

---
# Monte Carlo Simulations

1. Set population parameters

2. Create a random sample from these data

3. Do this multiple times 

4. Calculate how many times you get a significant result

  - E.g., 5 out 10 times (50% power)

---


```r
nsims=1000 # number of times we do this

power_result_vig_2 &lt;- ANOVA_power(design_result, 
                                  nsims = nsims, 
                                  seed = 1234)
```

```
## Power and Effect sizes for ANOVA tests
##                 power effect_size
## anova_condition  38.2      0.1461
## 
## Power and Effect sizes for pairwise comparisons (t-tests)
##                                      power effect_size
## p_condition_Cloak_condition_No Cloak  38.2     -0.7335
```

```r
#Note we do not specify any correlation in the ANOVA_design function (default r = 0), nor do we specify an alpha in the ANOVA_power function (default is 0.05)

knitr::kable(confint(power_result_vig_2, level = .98))
```



|                | power| lower.ci| upper.ci|
|:---------------|-----:|--------:|--------:|
|anova_condition |  38.2| 35.23915| 41.25117|
---
# Power Curve

```r
plot_power(design_result, min_n = 10, max_n = 250)
```

&lt;img src="eff_size_power_files/figure-html/unnamed-chunk-37-1.png" width="36%" style="display: block; margin: auto;" /&gt;

```
## Achieved Power and Sample Size for ANOVA-level effects
##    variable                  label  n achieved_power desired_power
## 1 condition Desired Power Achieved 44          90.12            90
```
---
# Dependent: Power

- The test type will affect power, as our independent results suggested we needed 44 or more people


```r
pwr.t.test(n = 12, 
           d = 1.10, 
           sig.level = .05,
           power = NULL, 
           type = "paired", 
           alternative = "two.sided")
```

```
## 
##      Paired t test power calculation 
## 
##               n = 12
##               d = 1.1
##       sig.level = 0.05
##           power = 0.9333861
##     alternative = two.sided
## 
## NOTE: n is number of *pairs*
```

---

```r
# if (!require("devtools")) {
 #   install.packages("devtools", dependencies = TRUE)}

 #  devtools::install_github("DejanDraschkow/mixedpower") 
   
library(mixedpower)
   
longdata$id&lt;-rep(1:12, length(longdata))

d_reg&lt;-lme4::lmer(Mischief~Cloak + (1|id),  data=longdata)

d_reg_mixed &lt;- mixedpower(d_reg, data=longdata, fixed_effects = c("Cloak"), simvar = "id", steps = c(20, 30, 40, 50, 60), critical_value = 2)
```
---
# Posthoc Power

- Sometimes reviewers will ask you to conduct a  post-experiment power calculation in order to interpret non-significant findings

  - Do not do this! 

1. Sample effect size not rep of population size

2. Adds nothing over a *p*-value

  - *p* &gt; .05  = low power(Duh!)

---

```r
set.seed(11)
x1 &lt;- rnorm(10, mean = 10, sd = 1)
x2 &lt;- rnorm(10, mean = 10.1, sd = 1)

#run t test
# get effect size
```
---
# Posthoc Power


```r
#calculate posthoc power from this
```

---
# Posthoc Power 
.pull-left[
&lt;br&gt;
&lt;br&gt;

```r
sim_out &lt;- replicate(n = 2000, expr = {
  x1 &lt;- rnorm(10, mean = 10, sd = 1)
  x2 &lt;- rnorm(10, mean = 10.1, sd = 1)
  ttest &lt;- t.test(x1, x2, var.equal = TRUE)
  pwr &lt;- power.t.test(delta = diff(ttest$estimate), 
               sd = sqrt((var(x1) + var(x2))/2), 
               sig.level = 0.05,
               n = 10)
  c(pvalue = ttest$p.value, obs_power = pwr$power)
})
```
]

.pull-right[
&lt;br&gt;
&lt;br&gt;


```r
plot(t(sim_out),
     xlim = c(0,1), ylim = c(0,1), 
     xlab = "p-value", ylab = "observed power",
     main = "Two-sample t test simulation")
```

&lt;img src="eff_size_power_files/figure-html/unnamed-chunk-43-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---
# The Minimal Detectable Effect Size 

- Report the smallest effect size that could be detected in your study for this particular sample size

- We do not live in a perfect world

  - Sometimes we cannot collect all the data we need

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "20:12",
"countIncrementalSlides": true,
"background-image": "url(\"lover.png\")",
"background-size": "cover"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
