---
title: "PSY 503: Foundations of Statistical Methods in Psychological Science"
subtitle: "More LM: Transformations"
institute: "Princeton University"
author: "Jason Geller, Ph.D. (he/him/his)"
date:  'Updated:`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
      background-image: url("lover.png")
      background-size: cover
      
---
```{r xaringan-extra-styles, echo=FALSE}
library(xaringanExtra)
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
```

```{html, echo=FALSE}
<div style = "position:fixed; visibility: hidden">
$$\require{color}\definecolor{yellow}{rgb}{1, 0.8, 0.16078431372549}$$
$$\require{color}\definecolor{orange}{rgb}{0.96078431372549, 0.525490196078431, 0.203921568627451}$$
$$\require{color}\definecolor{green}{rgb}{0, 0.474509803921569, 0.396078431372549}$$
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      yellow: ["{\\color{yellow}{#1}}", 1],
      orange: ["{\\color{orange}{#1}}", 1],
      green: ["{\\color{green}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
</script>
<style>
.yellow {color: #FFCC29;}
.orange {color: #F58634;}
.green {color: #007965;}
</style>
```{r, echo=FALSE}
library(flair)
yellow <- "#FFCC29"
orange <- "#F58634"
green <- "#007965"
```


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "50%",
  tidy.opts=list(width.cutoff=60),tidy=TRUE, 
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  fig.show = TRUE,
  hiline = TRUE
)
hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})
```

<div style = "position:fixed; visibility: hidden">
$$\require{color}\definecolor{red}{rgb}{1, 0, 0}$$
$$\require{color}\definecolor{green}{rgb}{0, 1, 0}$$
$$\require{color}\definecolor{blue}{rgb}{0, 0, 1}$$
</div>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      red: ["{\\color{red}{#1}}", 1],
      green: ["{\\color{green}{#1}}", 1],
      blue: ["{\\color{blue}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
</script>

<style>
.red {color: #FF0000;}
.green {color: #00FF00;}
.blue {color: #0000FF;}
</style>


```{r flair_color, echo=FALSE}
library(flair)
red <- "#FF0000"
green <- "#00FF00"
blue <- "#0000FF"
```


```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_solarized_dark(
  header_font_google = google_font("Work Sans"),
  header_h1_font_size = "36px",
  header_color = "black",
  text_font_google = google_font("Work Sans"),
  text_font_size = "30px",
  text_color = "black", 
  background_color = "white", 
  code_font_google = google_font("Share Tech Mono"),
  extra_css = list(
    ".remark-slide-content h2" = list(
      "margin-top" = "2em",
      "margin-bottom" = "2em"
    ),
    .big = list("font-size" = "150%"),
    .small = list("font-size" = "75%"),
    .subtle = list(opacity = "0.6"),
    ".countdown-has-style h3, .countdown-has-style h3 ~ p, .countdown-has-style h3 ~ ul" = list(
      "margin" = "0"
    ),
    ".countdown-has-style pre" = list(
      "margin-top" = "-10px"
    ),
    "p .remark-inline-code" = list(
      "background-color" = "white",
      "padding" = "2px 2px",
      "margin" = "0 -2px"
    ),
    blockquote = list("margin-left" = 0),
    "em" = list(color = "#2aa198")
  ),
)
```

```{r, echo=FALSE}
library(parameters)
library(effectsize) 
library(papaja)
library(tidyverse)
library(performance)
library(see)
library(equatiomatic)
library(kableExtra)
library(broom)
library(report)
library(flextable)
library(huxtable)
library(skimr)
library(papaja)

senses <- read_csv("https://raw.githubusercontent.com/jgeller112/psy503-psych_stats/master/static/slides/11-Categorical%20Predictors/data/winter_2016_senses_valence.csv")

data = read_csv("https://raw.githubusercontent.com/jgeller112/psy503-psych_stats/master/static/slides/10-linear_modeling/data/age_pitch.csv")

df <- mutate(data,
             age_c = age - mean(age, na.rm = TRUE), # center
             age_z = age_c / sd(age, na.rm = TRUE))# standardize

```
# Outline

- Check-In Q&A

- Transformations

  - Linear Transformations

     - Centering
     - Standardization
    
  - Nonlinear Transformations
  
    - Logarithms

---
# Check-In Q&A

1. Can a model be too good?

2. What is a grand mean?

3. Intercept of the linear model

4. In effects coding/sum coding, why is centering necessary? Why do we want to change 
the intercept to correspond to the overall mean?

5. What is a grand mean?

6. $\eta^2$ vs $\eta_p^2$

7. Would you ever use dummy coding with multiple levels? 

8. Heteroscedasticity
---
# Check-in Questions

1. How to avoid overfitting in your regression model?

--

.pull-left[

- General rule of thumb is 10-20 participants for every predictor variable in your model

- Use other methods to overfitiin like lasso or ridge regression

- Only include predictors that are theoretically meaningful
]

.pull-right[
```{r, echo=FALSE, fig.align='center', out.width="100%"}

knitr::include_graphics("Overfitting-1.png")
```
]
---
- Sum of Squares

```{r, echo=FALSE, fig.align='center', out.width="70%"}

knitr::include_graphics("aov_table.bmp")
```
---
# Restricted and Full Model

$$F = \frac{SS_{R}-SS_{F}/{df_{R}-df_{F}} (p-1)}{SS_{F}/df_F(N-p)} = \frac{MS_{model}}{MS_{error}}$$
```{r}

restricted <- lm(Val~ 1, data=senses) # intercept-only model

full <- lm(Val~Modality, data=senses)  # full model 

```

---
# Weekly Check-In Questions

3. Significance of the intercept of the linear model

- Generally speaking the *p*-value for the intercept tells us if the intercept is different from 0. Most often this is not of importance. 

$$H_0:\beta_0=0$$
---
# Weekly Check-In Questions

6. What is grand mean?

- It is the mean of the means

$$\frac{Group1_{mean} + Group2_{mean}}{2}$$
```{r}

senses %>%
  group_by(Modality) %>% # get each group
  dplyr::summarise(meanval=mean(Val))%>% # get mean
 dplyr::summarise(meanofmeans=mean(meanval)) # mean of means

```
---
# Weekly Check-In Questions

6. Eta-squared vs partial eta-squared 

$$\eta^2$$: % of total variance explained by IV

$$\eta_p^2$$: % of variance explained by IV partailing out other variables in the model

---
# Weekly Check-In Questions

7. Would you ever use dummy coding with multiple levels? 

```{r}
senses <- read_csv("https://raw.githubusercontent.com/jgeller112/psy503-psych_stats/master/static/slides/11-Categorical%20Predictors/data/winter_2016_senses_valence.csv")
lm(Val~Modality, data=senses) %>%
  model_parameters()
```
---
# Today's Datasets

  - Pitch and Age

```{r, message = FALSE, warning = FALSE}

data = read_csv("https://raw.githubusercontent.com/jgeller112/psy503-psych_stats/master/static/slides/10-linear_modeling/data/age_pitch.csv")

```

---
# Today's Datasets

- Memory and Time

 - 13 subjects were asked to memorize a list of disconnected items
 
 - The subjects were then asked to recall the items at various times up to a week later 

```{r}

log_df <- tibble::tribble(
   ~time, ~prop,
      1L,  0.84,
      5L,  0.71,
     15L,  0.61,
     30L,  0.56,
     60L,  0.54,
    120L,  0.47,
    240L,  0.45,
    480L,  0.38,
    720L,  0.36,
   1440L,  0.26,
   2880L,   0.2,
   5760L,  0.16,
  10080L,  0.08
  )
```
---
# Transformations

- Why/when should we transform our data?

  - Sometimes our data is *uninterpretable or nonsensical* 
  
--

  - Sometimes our data is *non-linear*

--

  - Sometimes our data is *skewed* 
---
# Linear Transformations

  - Adding, subtracting, dividing by, or multiplying a variable with a constant
    
  - Does not change the relationships in a genuine way (“same model”)
--

- Nonlinear transformations

    - Transformation that affects different data points differently
  
    - Changes the relationships (“different model”)
  
    - Logarithms 

---
# Linear Transformations

- Linear transformations

  - Adding, subtracting, dividing by, or multiplying a variable with a constant
  
  - Does not change the relationships in a genuine way (“same model”)
  
    - Common: centering and standardizing
---
# Transformations

- Nonlinear transformations

   - Transformation that affects different data points differently
  
  - Changes the relationships (“different model”)
  
    - Common: Logarthims 
    
  - Makes models more in line with assumptions
---
# Linear Transformations

```{r, fig.align='center', out.width="100%", echo=FALSE}
#manually
anim.1<- ggplot(data, aes(age,pitch))+
   geom_point(size=5)+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE, size=3)+
  theme_bw(base_size=16)

anim.1

```
---
# Linear Transformations

```{r, fig.align='center', out.width="100%", echo=FALSE}
#manually
anim.1<- ggplot(data, aes(age,pitch))+
   geom_point(size=5)+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE, size=3)+
  theme_bw(base_size=16) + 
    expand_limits(x = 0)


anim.1

```

---
```{r, fig.align='center', echo=FALSE, out.width="100%"}
#Animation
# Change the point sizes manually
anim.1<- ggplot(data, aes(age,pitch))+
  geom_point(size=5)+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE, size=5)+
  theme_bw(base_size=16) +
  geom_vline(xintercept=0, linetype="dotted") + 
  geom_point(x=0, y=216, colour="red", size=10, shape=22, fill="red") + 
  expand_limits(x = 0)

anim.1
```
---
# Centering

> Sometimes our data is *uninterpretable or nonsensical* (X = 0)

- Centering changes the model so that 0 means
something

  - 0 is now the average value of $x$ 

```{r, fig.align='center', echo=FALSE, out.width="100%"}
#Animation# Change the point sizes manually #center
data$age_c <- data$age-mean(data$age)

anim.1<- ggplot(data, aes(age_c,pitch))+
  geom_point(size=5)+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE, size=5)+
  theme_bw(base_size=20) +
  geom_vline(xintercept=0, linetype="dotted") + 
  geom_point(x=0, y=175, colour="red", size=10, shape=22, fill="red")+ 
  expand_limits(x = 0)

anim.1
```
---
# Centering - How to Do It

```{r}
df <- mutate(data,
             age_c = age - mean(age, na.rm = TRUE)) # center
```

```{r}
library(datawizard) # package to center and standardize
df_wiz <- data %>% 
  mutate(age_c = datawizard::center(age))
```
---
# Standardizing Predictor 

- Dividing centered mean by $\sigma$

- Puts variables on same metrics 

```{r, fig.align='center', echo=FALSE, out.width="100%"}
#Animation
# Change the point sizes manually
df_wiz <- data %>% 
  mutate(
          # center
             age_z = datawizard::standardize(age))

anim.1<- ggplot(df_wiz, aes(age_z,pitch))+
  geom_point(size=5)+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE, size=5)+
  theme_bw(base_size=20) +
  geom_vline(xintercept=0, linetype="dotted") + 
  geom_point(x=0, y=175, colour="red", size=10, shape=22, fill="red") + 
  expand_limits(x = 0)
  

anim.1
```
---
# Standardizing - How to Do It

```{r}
df_stand <- data %>%
  mutate(age_z = age_c / sd(age, na.rm = TRUE))# standardize
```

```{r, echo=FALSE}
df_wiz <- mutate(data,
             age_z = datawizard::standardize(age))

head(df_wiz)
```
---
Output

```{r}
lm(pitch~age, data=data)
lm(pitch~age_c, data=df)
lm(pitch~age_z, data=df_wiz)
```

- What has changed here?
- What has not changed?
---
# Centering

- Uncentering? 

  - Good if zero is *meaningful*
  
- Centering?

  - Good if zero is not meaningful
---
# Centering Around Other Values

- We could also make 0 correspond to some other sensible/useful value

  - Smallest logically possible value?

---
class: middle

# When in doubt, center!

---
class: middle

# Nonlinear Transformations

---
# Logathimthic Transformations 

```{r, fig.align='center', out.width="100%"}

knitr::include_graphics("log.bmp")

```
---
# Logathimthic Transformations 

```{r, fig.align='center', out.width="100%"}

knitr::include_graphics("base.PNG")

```
---
# Logathimthic Transformations 

- Exponentiation

- Takes small numbers and grows them 


$$10^1 = 10$$
$$10^2 = 100$$
$$10^3 = 1000$$
$$10^4 = 10000$$
$$10^5 = 100000$$
---
# Log Transformations

- Logarithmic

- Takes large numbers and shrinks them 

$$1 = log_{10}(10)$$
$$2 = log_{10}(100)$$
$$3 = log_{10}(1000)$$
$$4 = log_{10}(10000)$$
$$5 = log_{10}(100000)$$
---
# Log Transfomrations

- Tracks the order of magnitude

- Large numbers shrink more than smaller numbers

  - Compression effect
  
```{r}
RTs <- c(600, 650, 700, 1000, 4000)


```

---
# Predictor ($X$) Transformations

- Make more linear

- Lets plot our memory data

```{r, fig.align='center', out.width="100%", echo=FALSE}
#manually
anim.1<- ggplot(log_df, aes(time,prop))+
   geom_point(size=5)+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE, size=5)+
  theme_bw(base_size=18) + 
  expand_limits(x = 0)

anim.1

```

---
# Other checks

```{r, fig.align='center', out.width="100%"}
lm(prop~time, data=log_df)%>%
check_model()
```

---
# Fix?

- Log(time)

```{r}
log_df <- log_df %>% 
  mutate(log_time=log(time))
```

---
# Check Again

- Much better!

```{r, fig.align='center', out.width="100%", echo=FALSE}
#manually
anim.1<- ggplot(log_df, aes(log_time,prop))+
   geom_point(size=5)+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE, size=5)+
  theme_bw(base_size=18) + 
   expand_limits(x = 0)
anim.1

```
---
# Check Assumptions Again 

```{r, fig.align='center', out.width="100%"}
lm(prop~log_time, data=log_df)%>%
check_model()
```

---
# Let's Analyze 

```{r, fig.align='center', out.width="100%"}
lm(prop~log_time, data=log_df) %>%
  report()

```
---
# Interpretation of Log transformation

- When using logarithms, you model percentage increase or decrease instead of absolute differences

- Not: RTs are 40 ms slower in incongruent trials

- But: RTs are 7% slower in incongruent trials

---
# Outcome (Y) Transformations

-  Log RTs

```{r, fig.align='center', out.width="100%"}

knitr::include_graphics("log.bmp")

```

---
# Outcome (Y) Transformations

- Square root ($\sqrt(y)$)

- Inverse transformation (1/y)

  - Makes interpretation hard! 
---
# Rules for Interpretation

- OK, so you used a log-transform

- Only the dependent/response variable?

  - Exponentiate the coefficient, subtract one from this number, and multiply by 100. This gives the percent increase (or decrease) in the response for every one-unit increase in the independent variable. Example: the coefficient is 0.198. (exp(0.198) – 1) * 100 = 21.9. For every one-unit increase in the independent variable, our dependent variable increases by about 22%
  
- Only independent/predictor variable(s) is log-transformed?

  - Divide the coefficient by 100. This tells us that a 1% increase in the independent variable increases (or decreases) the dependent variable by (coefficient/100) units. Example: the coefficient is 0.198. 0.198/100 = 0.00198. For every 1% increase in the independent variable, our dependent variable increases by about 0.002. For x percent increase, multiply the coefficient by log(1.x). Example: For every 10% increase in the independent variable, our dependent variable increases by about 0.198 * log(1.10) = 0.02
---
# Rules for Interpretation

- Both dependent/response variable and independent/predictor variable(s) are log-transformed?

  - Interpret the coefficient as the percent increase in the dependent variable for every 1% increase in the independent variable. Example: the coefficient is 0.198. For every 1% increase in the independent variable, our dependent variable increases by about 0.20%. For x percent increase, calculate 1.x to the power of the coefficient, subtract 1, and multiply by 100. Example: For every 20% increase in the independent variable, our dependent variable increases by about (1.20 0.198 – 1) * 100 = 3.7 percent.

---
# Let's Analyze 

```{r, fig.align='center', out.width="100%"}
lm(prop~log_time, data=log_df) %>%
  report()

```
---
# When Should you Log Transform?

- Ideally: When it’s theoretically motivated

- After you look at the relationship to DV 

- Common in linguistics and psychology:

  - Frequency
  - Response times
  - Perceptual magnitudes
  
- *If you want/need to center, apply log transform after!*
---
# Other Data Transformations

- Square root $\sqrt(y)$

- Inverse transformation (1/y)

  - Makes interpretation hard! 
---
# Data Transformations

1. If the primary problem with your model is non-linearity, look at a scatter plot of the data to suggest transformations that might help

2. If the variances are unequal and/or error terms are not normal, try a "power transformation"

  - Box-Cox Transformation 

3. Be transparent!

---
class: middle 

# Polynomial Models

---

# Polynomial Models

.pull-right[

-  A nonlinear method that models the relationship between X and Y using polynomials

- Polynomial is mathematical expression
of operators and non-negative powers
]

.pull-right[
```{r}

knitr::include_graphics("orthogonal-curves-1.png")

```
]
---
# Memory Example

```{r, fig.align='center', out.width="100%", echo=FALSE}
#manually
anim.1<- ggplot(log_df, aes(time,prop))+
   geom_point(size=5)+
  theme_bw(base_size=18)
anim.1

```
---
# Testing in Polynomial Models

- Lower-order terms must always remain in model 

- Testing only the highest order  
$x$ terms

- Testing all of the terms

- Testing some of the x terms, starting with the highest order term and including the next lower-order terms from there.
---

$$\begin{equation}
Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3x_i^2 + \epsilon_i.
\end{equation}$$

$$H_0: \beta_3 = 0$$

$$H_0: \beta_1 = \beta_2 = \beta_3 = 0$$

$$H_0: \beta_2 = \beta_3 = 0$$

---
# Polynomial Regression

$$\begin{equation}
Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \epsilon_i.
\end{equation}$$
```{r}

log_df_quad <- log_df%>%
  mutate(time2=time^2) # add in quadratic
  
lm(prop ~ time + time2,  data=log_df_quad) %>%
  model_parameters()

```
---
# Model Fit Quadratic

```{r}
anim.1<- ggplot(log_df, aes(time,prop))+
   geom_point(size=5)+
  theme_bw(base_size=18)+ 
 stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1)

```
---
# Model Fit Comparison

```{r}

lm_lin <- lm(prop ~ time,  data=log_df_quad) 

lm_quad <- lm(prop ~ time + time2,  data=log_df_quad)

anova(lm_lin, lm_quad)

```


